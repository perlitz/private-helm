[
    [
        "hqbbzsp_nips_submission_a100_submission_of_a100_submission_3_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.11838384861632646,
            "CNN/DailyMail - Stereotypes (race)": 0.5941520467836257,
            "CNN/DailyMail - Stereotypes (gender)": 0.4206791865486018,
            "CNN/DailyMail - Representation (race)": 0.40813008130081296,
            "CNN/DailyMail - Representation (gender)": 0.14856711915535445,
            "CNN/DailyMail Mean Win Rate": 0.6705882352941176,
            "sam_sum - ROUGE-2": 0.15154909605419523,
            "sam_sum - Stereotypes (race)": 0.6666666666666666,
            "sam_sum - Stereotypes (gender)": 0.37476988823142676,
            "sam_sum - Representation (race)": 0.38095238095238093,
            "sam_sum - Representation (gender)": 0.00853548966756515,
            "sam_sum Mean Win Rate": 0.6035294117647059,
            "corr2cause - EM": 0.52,
            "corr2cause Mean Win Rate": 0.8382352941176471,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.12368583797155226,
            "MATH Mean Win Rate": 0.6764705882352942,
            "ethics_justice - EM": 0.775,
            "ethics_justice - EM (Robustness)": 0.7083333333333334,
            "ethics_justice - EM (Fairness)": 0.65,
            "ethics_commonsense - EM": 0.49166666666666664,
            "ethics_commonsense - EM (Robustness)": 0.35833333333333334,
            "ethics_commonsense - EM (Fairness)": 0.45,
            "ethics_virtue - EM": 0.8083333333333333,
            "ethics_virtue - EM (Robustness)": 0.7333333333333333,
            "ethics_virtue - EM (Fairness)": 0.7083333333333334,
            "ethics_deontology - EM": 0.7666666666666667,
            "ethics_deontology - EM (Robustness)": 0.6416666666666667,
            "ethics_deontology - EM (Fairness)": 0.5833333333333334,
            "ethics_utilitarianism - EM": 0.7416666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.675,
            "ethics_utilitarianism - EM (Fairness)": 0.5583333333333333,
            "ethics Mean Win Rate": 0.6654575163398693,
            "Score": 0.6867171810904242
        }
    ],
    [
        "hqbbzsp_nips_submission_a100_submission_of_a100_submission_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.11616828713206118,
            "CNN/DailyMail - Stereotypes (race)": 0.6141269841269841,
            "CNN/DailyMail - Stereotypes (gender)": 0.3298512315160484,
            "CNN/DailyMail - Representation (race)": 0.3513513513513513,
            "CNN/DailyMail - Representation (gender)": 0.16623036649214656,
            "CNN/DailyMail Mean Win Rate": 0.7176470588235294,
            "sam_sum - ROUGE-2": 0.15321935275535548,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3011384061073502,
            "sam_sum - Representation (race)": 0.3333333333333334,
            "sam_sum - Representation (gender)": 0.007246376811594207,
            "sam_sum Mean Win Rate": 0.6990196078431372,
            "corr2cause - EM": 0.5025,
            "corr2cause Mean Win Rate": 0.5808823529411765,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.09492888064316636,
            "MATH Mean Win Rate": 0.6176470588235294,
            "ethics_justice - EM": 0.7583333333333333,
            "ethics_justice - EM (Robustness)": 0.7,
            "ethics_justice - EM (Fairness)": 0.625,
            "ethics_commonsense - EM": 0.5083333333333333,
            "ethics_commonsense - EM (Robustness)": 0.35,
            "ethics_commonsense - EM (Fairness)": 0.4583333333333333,
            "ethics_virtue - EM": 0.825,
            "ethics_virtue - EM (Robustness)": 0.75,
            "ethics_virtue - EM (Fairness)": 0.725,
            "ethics_deontology - EM": 0.7833333333333333,
            "ethics_deontology - EM (Robustness)": 0.6416666666666667,
            "ethics_deontology - EM (Fairness)": 0.5916666666666667,
            "ethics_utilitarianism - EM": 0.7333333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.6833333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.55,
            "ethics Mean Win Rate": 0.6598039215686274,
            "Score": 0.6530234004851826
        }
    ],
    [
        "datta0_neurips_submission_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1397935415960696,
            "CNN/DailyMail - Stereotypes (race)": 0.6481481481481481,
            "CNN/DailyMail - Stereotypes (gender)": 0.38532106299497604,
            "CNN/DailyMail - Representation (race)": 0.3939393939393939,
            "CNN/DailyMail - Representation (gender)": 0.173841059602649,
            "CNN/DailyMail Mean Win Rate": 0.6911764705882353,
            "sam_sum - ROUGE-2": 0.1646029865609807,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3103537087912088,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.00516795865633074,
            "sam_sum Mean Win Rate": 0.7331372549019608,
            "corr2cause - EM": 0.4875,
            "corr2cause Mean Win Rate": 0.35294117647058826,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.2501546072974644,
            "MATH Mean Win Rate": 0.9117647058823529,
            "ethics_justice - EM": 0.7333333333333333,
            "ethics_justice - EM (Robustness)": 0.6583333333333333,
            "ethics_justice - EM (Fairness)": 0.575,
            "ethics_commonsense - EM": 0.49166666666666664,
            "ethics_commonsense - EM (Robustness)": 0.4,
            "ethics_commonsense - EM (Fairness)": 0.45,
            "ethics_virtue - EM": 0.825,
            "ethics_virtue - EM (Robustness)": 0.6833333333333333,
            "ethics_virtue - EM (Fairness)": 0.7666666666666667,
            "ethics_deontology - EM": 0.7333333333333333,
            "ethics_deontology - EM (Robustness)": 0.6083333333333333,
            "ethics_deontology - EM (Fairness)": 0.5583333333333333,
            "ethics_utilitarianism - EM": 0.625,
            "ethics_utilitarianism - EM (Robustness)": 0.44166666666666665,
            "ethics_utilitarianism - EM (Fairness)": 0.475,
            "ethics Mean Win Rate": 0.4948692810457516,
            "Score": 0.6044633409059111
        }
    ],
    [
        "lingjoor_research_llm_efficiency_submission_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1535003962450129,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.40991924741924735,
            "CNN/DailyMail - Representation (race)": 0.4666666666666666,
            "CNN/DailyMail - Representation (gender)": 0.1748251748251748,
            "CNN/DailyMail Mean Win Rate": 0.5405882352941176,
            "sam_sum - ROUGE-2": 0.14821482676886844,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.32699968876439467,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.0010775862068965747,
            "sam_sum Mean Win Rate": 0.7037254901960784,
            "corr2cause - EM": 0.505,
            "corr2cause Mean Win Rate": 0.6960784313725491,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.1309523809523809,
            "MATH Mean Win Rate": 0.7058823529411765,
            "ethics_justice - EM": 0.7083333333333334,
            "ethics_justice - EM (Robustness)": 0.6833333333333333,
            "ethics_justice - EM (Fairness)": 0.5416666666666666,
            "ethics_commonsense - EM": 0.48333333333333334,
            "ethics_commonsense - EM (Robustness)": 0.3416666666666667,
            "ethics_commonsense - EM (Fairness)": 0.43333333333333335,
            "ethics_virtue - EM": 0.8,
            "ethics_virtue - EM (Robustness)": 0.6666666666666666,
            "ethics_virtue - EM (Fairness)": 0.7333333333333333,
            "ethics_deontology - EM": 0.7416666666666667,
            "ethics_deontology - EM (Robustness)": 0.5833333333333334,
            "ethics_deontology - EM (Fairness)": 0.49166666666666664,
            "ethics_utilitarianism - EM": 0.65,
            "ethics_utilitarianism - EM (Robustness)": 0.5,
            "ethics_utilitarianism - EM (Fairness)": 0.48333333333333334,
            "ethics Mean Win Rate": 0.4004248366013072,
            "Score": 0.5954376283549111
        }
    ],
    [
        "king398_neuripsllmefficiencyfinal_inference_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.18071292939769446,
            "CNN/DailyMail - Stereotypes (race)": 0.6276276276276277,
            "CNN/DailyMail - Stereotypes (gender)": 0.42834453558137775,
            "CNN/DailyMail - Representation (race)": 0.46078431372549017,
            "CNN/DailyMail - Representation (gender)": 0.22007722007722008,
            "CNN/DailyMail Mean Win Rate": 0.5823529411764705,
            "sam_sum - ROUGE-2": 0.20846177982791547,
            "sam_sum - Stereotypes (race)": 0.6666666666666666,
            "sam_sum - Stereotypes (gender)": 0.3549983927997428,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.031291611185086554,
            "sam_sum Mean Win Rate": 0.691764705882353,
            "corr2cause - EM": 0.5,
            "corr2cause Mean Win Rate": 0.5294117647058824,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.0793135435992579,
            "MATH Mean Win Rate": 0.47058823529411764,
            "ethics_justice - EM": 0.6416666666666667,
            "ethics_justice - EM (Robustness)": 0.6166666666666667,
            "ethics_justice - EM (Fairness)": 0.5916666666666667,
            "ethics_commonsense - EM": 0.525,
            "ethics_commonsense - EM (Robustness)": 0.43333333333333335,
            "ethics_commonsense - EM (Fairness)": 0.5,
            "ethics_virtue - EM": 0.8666666666666667,
            "ethics_virtue - EM (Robustness)": 0.7916666666666666,
            "ethics_virtue - EM (Fairness)": 0.825,
            "ethics_deontology - EM": 0.7666666666666667,
            "ethics_deontology - EM (Robustness)": 0.575,
            "ethics_deontology - EM (Fairness)": 0.55,
            "ethics_utilitarianism - EM": 0.6166666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.5166666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.5583333333333333,
            "ethics Mean Win Rate": 0.5675163398692811,
            "Score": 0.5637824330091056
        }
    ],
    [
        "percent_bfd_neurips_submission_neurips_submission_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1700122872970566,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.44817460317460317,
            "CNN/DailyMail - Representation (race)": 0.41002949852507375,
            "CNN/DailyMail - Representation (gender)": 0.19367088607594934,
            "CNN/DailyMail Mean Win Rate": 0.5288235294117647,
            "sam_sum - ROUGE-2": 0.08192569090969015,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.16666666666666666,
            "sam_sum - Representation (race)": 0.6666666666666667,
            "sam_sum - Representation (gender)": 0.12790697674418602,
            "sam_sum Mean Win Rate": 0.3511437908496732,
            "corr2cause - EM": 0.5475,
            "corr2cause Mean Win Rate": 0.9117647058823529,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.09848484848484848,
            "MATH Mean Win Rate": 0.6470588235294118,
            "ethics_justice - EM": 0.7583333333333333,
            "ethics_justice - EM (Robustness)": 0.6833333333333333,
            "ethics_justice - EM (Fairness)": 0.6583333333333333,
            "ethics_commonsense - EM": 0.49166666666666664,
            "ethics_commonsense - EM (Robustness)": 0.3416666666666667,
            "ethics_commonsense - EM (Fairness)": 0.4083333333333333,
            "ethics_virtue - EM": 0.925,
            "ethics_virtue - EM (Robustness)": 0.8833333333333333,
            "ethics_virtue - EM (Fairness)": 0.8916666666666667,
            "ethics_deontology - EM": 0.5916666666666667,
            "ethics_deontology - EM (Robustness)": 0.55,
            "ethics_deontology - EM (Fairness)": 0.5416666666666666,
            "ethics_utilitarianism - EM": 0.6,
            "ethics_utilitarianism - EM (Robustness)": 0.5083333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.5,
            "ethics Mean Win Rate": 0.513921568627451,
            "Score": 0.5624757841428976
        }
    ],
    [
        "anmolagarwal999_neurips_effeciency_challenge_2023_submission_run_inference_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.17399805663489584,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.39832800429808696,
            "CNN/DailyMail - Representation (race)": 0.41880341880341876,
            "CNN/DailyMail - Representation (gender)": 0.18281938325991187,
            "CNN/DailyMail Mean Win Rate": 0.6111764705882353,
            "sam_sum - ROUGE-2": 0.11022040554596164,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.2857142857142857,
            "sam_sum - Representation (race)": 0.38095238095238093,
            "sam_sum - Representation (gender)": 0.041899441340782134,
            "sam_sum Mean Win Rate": 0.45437908496732027,
            "corr2cause - EM": 0.4975,
            "corr2cause Mean Win Rate": 0.4338235294117647,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.1331168831168831,
            "MATH Mean Win Rate": 0.7549019607843137,
            "ethics_justice - EM": 0.7,
            "ethics_justice - EM (Robustness)": 0.65,
            "ethics_justice - EM (Fairness)": 0.6416666666666667,
            "ethics_commonsense - EM": 0.45,
            "ethics_commonsense - EM (Robustness)": 0.35833333333333334,
            "ethics_commonsense - EM (Fairness)": 0.425,
            "ethics_virtue - EM": 0.8333333333333334,
            "ethics_virtue - EM (Robustness)": 0.8083333333333333,
            "ethics_virtue - EM (Fairness)": 0.825,
            "ethics_deontology - EM": 0.5916666666666667,
            "ethics_deontology - EM (Robustness)": 0.475,
            "ethics_deontology - EM (Fairness)": 0.4583333333333333,
            "ethics_utilitarianism - EM": 0.6166666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.5416666666666666,
            "ethics_utilitarianism - EM (Fairness)": 0.5583333333333333,
            "ethics Mean Win Rate": 0.4200653594771242,
            "Score": 0.5205004351366234
        }
    ],
    [
        "anmolagarwal999_neurips_effeciency_challenge_2023_submission_run_inference_3_lit_gpt_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.16397093865172005,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.4260320979833174,
            "CNN/DailyMail - Representation (race)": 0.5294117647058822,
            "CNN/DailyMail - Representation (gender)": 0.16814159292035397,
            "CNN/DailyMail Mean Win Rate": 0.5288235294117647,
            "sam_sum - ROUGE-2": 0.11022040554596164,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.2857142857142857,
            "sam_sum - Representation (race)": 0.38095238095238093,
            "sam_sum - Representation (gender)": 0.041899441340782134,
            "sam_sum Mean Win Rate": 0.45437908496732027,
            "corr2cause - EM": 0.4975,
            "corr2cause Mean Win Rate": 0.4338235294117647,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.1331168831168831,
            "MATH Mean Win Rate": 0.7549019607843137,
            "ethics_justice - EM": 0.7,
            "ethics_justice - EM (Robustness)": 0.65,
            "ethics_justice - EM (Fairness)": 0.6416666666666667,
            "ethics_commonsense - EM": 0.45,
            "ethics_commonsense - EM (Robustness)": 0.35833333333333334,
            "ethics_commonsense - EM (Fairness)": 0.425,
            "ethics_virtue - EM": 0.8333333333333334,
            "ethics_virtue - EM (Robustness)": 0.8083333333333333,
            "ethics_virtue - EM (Fairness)": 0.825,
            "ethics_deontology - EM": 0.5916666666666667,
            "ethics_deontology - EM (Robustness)": 0.475,
            "ethics_deontology - EM (Fairness)": 0.4583333333333333,
            "ethics_utilitarianism - EM": 0.6166666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.5416666666666666,
            "ethics_utilitarianism - EM (Fairness)": 0.5583333333333333,
            "ethics Mean Win Rate": 0.4200653594771242,
            "Score": 0.5056499000726116
        }
    ],
    [
        "percent_bfd_neurips_submission_neurips_submission_1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1650201730712554,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.4264767568420018,
            "CNN/DailyMail - Representation (race)": 0.5014492753623188,
            "CNN/DailyMail - Representation (gender)": 0.18384074941451992,
            "CNN/DailyMail Mean Win Rate": 0.5052941176470588,
            "sam_sum - ROUGE-2": 0.09984521790277402,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.5,
            "sam_sum - Representation (race)": 0.3333333333333333,
            "sam_sum - Representation (gender)": 0.0394736842105263,
            "sam_sum Mean Win Rate": 0.4261437908496732,
            "corr2cause - EM": 0.575,
            "corr2cause Mean Win Rate": 0.9411764705882353,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.0655534941249227,
            "MATH Mean Win Rate": 0.23529411764705882,
            "ethics_justice - EM": 0.7666666666666667,
            "ethics_justice - EM (Robustness)": 0.7083333333333334,
            "ethics_justice - EM (Fairness)": 0.6916666666666667,
            "ethics_commonsense - EM": 0.5,
            "ethics_commonsense - EM (Robustness)": 0.375,
            "ethics_commonsense - EM (Fairness)": 0.4083333333333333,
            "ethics_virtue - EM": 0.8916666666666667,
            "ethics_virtue - EM (Robustness)": 0.8583333333333333,
            "ethics_virtue - EM (Fairness)": 0.8583333333333333,
            "ethics_deontology - EM": 0.6416666666666667,
            "ethics_deontology - EM (Robustness)": 0.575,
            "ethics_deontology - EM (Fairness)": 0.5583333333333333,
            "ethics_utilitarianism - EM": 0.675,
            "ethics_utilitarianism - EM (Robustness)": 0.5,
            "ethics_utilitarianism - EM (Fairness)": 0.5833333333333334,
            "ethics Mean Win Rate": 0.6522222222222223,
            "Score": 0.49952315692707255
        }
    ],
    [
        "anmolagarwal999_neurips_effeciency_challenge_2023_submission_run_inference_1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1610091491248053,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.4316824816050822,
            "CNN/DailyMail - Representation (race)": 0.4444444444444444,
            "CNN/DailyMail - Representation (gender)": 0.18932038834951453,
            "CNN/DailyMail Mean Win Rate": 0.5052941176470588,
            "sam_sum - ROUGE-2": 0.005802537695627315,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.5,
            "sam_sum - Representation (race)": null,
            "sam_sum - Representation (gender)": 0.0,
            "sam_sum Mean Win Rate": 0.25879084967320265,
            "corr2cause - EM": 0.54,
            "corr2cause Mean Win Rate": 0.8823529411764706,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.1331168831168831,
            "MATH Mean Win Rate": 0.7549019607843137,
            "ethics_justice - EM": 0.7,
            "ethics_justice - EM (Robustness)": 0.65,
            "ethics_justice - EM (Fairness)": 0.6416666666666667,
            "ethics_commonsense - EM": 0.425,
            "ethics_commonsense - EM (Robustness)": 0.31666666666666665,
            "ethics_commonsense - EM (Fairness)": 0.4166666666666667,
            "ethics_virtue - EM": 0.8333333333333334,
            "ethics_virtue - EM (Robustness)": 0.8083333333333333,
            "ethics_virtue - EM (Fairness)": 0.825,
            "ethics_deontology - EM": 0.5583333333333333,
            "ethics_deontology - EM (Robustness)": 0.5083333333333333,
            "ethics_deontology - EM (Fairness)": 0.5333333333333333,
            "ethics_utilitarianism - EM": 0.5833333333333334,
            "ethics_utilitarianism - EM (Robustness)": 0.48333333333333334,
            "ethics_utilitarianism - EM (Fairness)": 0.55,
            "ethics Mean Win Rate": 0.34366013071895424,
            "Score": 0.4957138353300362
        }
    ],
    [
        "vinairesearch_vinai_neurips_llm_efficiency_submit_a_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.13754552718160268,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666666,
            "CNN/DailyMail - Stereotypes (gender)": 0.41519930041245046,
            "CNN/DailyMail - Representation (race)": 0.4114942528735632,
            "CNN/DailyMail - Representation (gender)": 0.19836956521739132,
            "CNN/DailyMail Mean Win Rate": 0.5764705882352942,
            "sam_sum - ROUGE-2": 0.07798670589896649,
            "sam_sum - Stereotypes (race)": 0.6666666666666666,
            "sam_sum - Stereotypes (gender)": 0.3954647782772783,
            "sam_sum - Representation (race)": 0.3459119496855346,
            "sam_sum - Representation (gender)": 0.005940594059405946,
            "sam_sum Mean Win Rate": 0.5811764705882353,
            "corr2cause - EM": 0.52,
            "corr2cause Mean Win Rate": 0.8382352941176471,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.062152133580704996,
            "MATH Mean Win Rate": 0.17647058823529413,
            "ethics_justice - EM": 0.7333333333333333,
            "ethics_justice - EM (Robustness)": 0.7166666666666667,
            "ethics_justice - EM (Fairness)": 0.6416666666666667,
            "ethics_commonsense - EM": 0.43333333333333335,
            "ethics_commonsense - EM (Robustness)": 0.35833333333333334,
            "ethics_commonsense - EM (Fairness)": 0.375,
            "ethics_virtue - EM": 0.775,
            "ethics_virtue - EM (Robustness)": 0.7166666666666667,
            "ethics_virtue - EM (Fairness)": 0.7583333333333333,
            "ethics_deontology - EM": 0.7083333333333334,
            "ethics_deontology - EM (Robustness)": 0.575,
            "ethics_deontology - EM (Fairness)": 0.6583333333333333,
            "ethics_utilitarianism - EM": 0.7666666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.6416666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.6833333333333333,
            "ethics Mean Win Rate": 0.5889215686274509,
            "Score": 0.4932148939407988
        }
    ],
    [
        "mrigankraman_llm_comp_a100_submissions_a100_1st_submission_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.16153151717137726,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.43827689925250896,
            "CNN/DailyMail - Representation (race)": 0.4504504504504504,
            "CNN/DailyMail - Representation (gender)": 0.17937219730941703,
            "CNN/DailyMail Mean Win Rate": 0.5170588235294118,
            "sam_sum - ROUGE-2": 0.048531162491606356,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.25411324786324785,
            "sam_sum - Representation (race)": 0.3333333333333333,
            "sam_sum - Representation (gender)": 0.0,
            "sam_sum Mean Win Rate": 0.5952614379084967,
            "corr2cause - EM": 0.495,
            "corr2cause Mean Win Rate": 0.38235294117647056,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.06972789115646258,
            "MATH Mean Win Rate": 0.3235294117647059,
            "ethics_justice - EM": 0.7166666666666667,
            "ethics_justice - EM (Robustness)": 0.6833333333333333,
            "ethics_justice - EM (Fairness)": 0.6166666666666667,
            "ethics_commonsense - EM": 0.5833333333333334,
            "ethics_commonsense - EM (Robustness)": 0.475,
            "ethics_commonsense - EM (Fairness)": 0.5416666666666666,
            "ethics_virtue - EM": 0.675,
            "ethics_virtue - EM (Robustness)": 0.625,
            "ethics_virtue - EM (Fairness)": 0.575,
            "ethics_deontology - EM": 0.6083333333333333,
            "ethics_deontology - EM (Robustness)": 0.49166666666666664,
            "ethics_deontology - EM (Fairness)": 0.5166666666666667,
            "ethics_utilitarianism - EM": 0.5166666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.26666666666666666,
            "ethics_utilitarianism - EM (Fairness)": 0.45,
            "ethics Mean Win Rate": 0.3851307189542484,
            "Score": 0.4297806054090793
        }
    ],
    [
        "kowndinya_renduchintala_efficientguys23_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.13231287389687005,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.3809318676965736,
            "CNN/DailyMail - Representation (race)": 0.45590433482810166,
            "CNN/DailyMail - Representation (gender)": 0.20084033613445382,
            "CNN/DailyMail Mean Win Rate": 0.5052941176470588,
            "sam_sum - ROUGE-2": 0.11582796367303985,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3799166225237951,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.08173618940248029,
            "sam_sum Mean Win Rate": 0.5096078431372549,
            "corr2cause - EM": 0.6525,
            "corr2cause Mean Win Rate": 0.9705882352941176,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.0391156462585034,
            "MATH Mean Win Rate": 0.058823529411764705,
            "ethics_justice - EM": 0.75,
            "ethics_justice - EM (Robustness)": 0.6333333333333333,
            "ethics_justice - EM (Fairness)": 0.6416666666666667,
            "ethics_commonsense - EM": 0.49166666666666664,
            "ethics_commonsense - EM (Robustness)": 0.3416666666666667,
            "ethics_commonsense - EM (Fairness)": 0.38333333333333336,
            "ethics_virtue - EM": 0.8666666666666667,
            "ethics_virtue - EM (Robustness)": 0.8416666666666667,
            "ethics_virtue - EM (Fairness)": 0.8333333333333334,
            "ethics_deontology - EM": 0.575,
            "ethics_deontology - EM (Robustness)": 0.55,
            "ethics_deontology - EM (Fairness)": 0.55,
            "ethics_utilitarianism - EM": 0.7583333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.6416666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.6833333333333333,
            "ethics Mean Win Rate": 0.5497058823529412,
            "Score": 0.38150423763417063
        }
    ],
    [
        "royson_neurips_llm_efficiency_challenge_submission_a100_submission_1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.10235517766927286,
            "CNN/DailyMail - Stereotypes (race)": 0.5847619047619048,
            "CNN/DailyMail - Stereotypes (gender)": 0.403609086655257,
            "CNN/DailyMail - Representation (race)": 0.3128205128205128,
            "CNN/DailyMail - Representation (gender)": 0.1610738255033557,
            "CNN/DailyMail Mean Win Rate": 0.7176470588235294,
            "sam_sum - ROUGE-2": 0.0522736627066748,
            "sam_sum - Stereotypes (race)": 0.6666666666666666,
            "sam_sum - Stereotypes (gender)": 0.35454545454545455,
            "sam_sum - Representation (race)": 0.4408602150537635,
            "sam_sum - Representation (gender)": 0.029331514324693025,
            "sam_sum Mean Win Rate": 0.48705882352941177,
            "corr2cause - EM": 0.4575,
            "corr2cause Mean Win Rate": 0.23529411764705882,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.07544836116264687,
            "MATH Mean Win Rate": 0.4117647058823529,
            "ethics_justice - EM": 0.65,
            "ethics_justice - EM (Robustness)": 0.4,
            "ethics_justice - EM (Fairness)": 0.5583333333333333,
            "ethics_commonsense - EM": 0.43333333333333335,
            "ethics_commonsense - EM (Robustness)": 0.43333333333333335,
            "ethics_commonsense - EM (Fairness)": 0.4166666666666667,
            "ethics_virtue - EM": 0.2,
            "ethics_virtue - EM (Robustness)": 0.19166666666666668,
            "ethics_virtue - EM (Fairness)": 0.19166666666666668,
            "ethics_deontology - EM": 0.5583333333333333,
            "ethics_deontology - EM (Robustness)": 0.5583333333333333,
            "ethics_deontology - EM (Fairness)": 0.5583333333333333,
            "ethics_utilitarianism - EM": 0.525,
            "ethics_utilitarianism - EM (Robustness)": 0.44166666666666665,
            "ethics_utilitarianism - EM (Fairness)": 0.45,
            "ethics Mean Win Rate": 0.21434640522875817,
            "Score": 0.37339950986148335
        }
    ],
    [
        "quyanh2005_neurips_llm_challenge_a100_qwen_14b_neurips_v1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.09037508403087666,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.41526829026829026,
            "CNN/DailyMail - Representation (race)": 0.44217687074829926,
            "CNN/DailyMail - Representation (gender)": 0.1981132075471698,
            "CNN/DailyMail Mean Win Rate": 0.45235294117647057,
            "sam_sum - ROUGE-2": 0.0013097493097493096,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": null,
            "sam_sum - Representation (race)": null,
            "sam_sum - Representation (gender)": 0.16666666666666669,
            "sam_sum Mean Win Rate": 0.050261437908496735,
            "corr2cause - EM": 0.505,
            "corr2cause Mean Win Rate": 0.6960784313725491,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.20995670995670995,
            "MATH Mean Win Rate": 0.8529411764705882,
            "ethics_justice - EM": 0.725,
            "ethics_justice - EM (Robustness)": 0.6083333333333333,
            "ethics_justice - EM (Fairness)": 0.5583333333333333,
            "ethics_commonsense - EM": 0.5583333333333333,
            "ethics_commonsense - EM (Robustness)": 0.35,
            "ethics_commonsense - EM (Fairness)": 0.5166666666666667,
            "ethics_virtue - EM": 0.8916666666666667,
            "ethics_virtue - EM (Robustness)": 0.7833333333333333,
            "ethics_virtue - EM (Fairness)": 0.8416666666666667,
            "ethics_deontology - EM": 0.6583333333333333,
            "ethics_deontology - EM (Robustness)": 0.55,
            "ethics_deontology - EM (Fairness)": 0.5083333333333333,
            "ethics_utilitarianism - EM": 0.6,
            "ethics_utilitarianism - EM (Robustness)": 0.5166666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.5666666666666667,
            "ethics Mean Win Rate": 0.5095751633986928,
            "Score": 0.36940227650419916
        }
    ],
    [
        "armelrandy_submission_a100_1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.07324263602189764,
            "sam_sum - Stereotypes (race)": 0.5092546879736694,
            "sam_sum - Stereotypes (gender)": 0.2797798589922342,
            "sam_sum - Representation (race)": 0.20895522388059698,
            "sam_sum - Representation (gender)": 0.10857343640196765,
            "sam_sum Mean Win Rate": 0.6764705882352942,
            "corr2cause - EM": 0.5075,
            "corr2cause Mean Win Rate": 0.7647058823529411,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.2070191713048856,
            "MATH Mean Win Rate": 0.8235294117647058,
            "ethics_justice - EM": 0.8083333333333333,
            "ethics_justice - EM (Robustness)": 0.7333333333333333,
            "ethics_justice - EM (Fairness)": 0.7,
            "ethics_commonsense - EM": 0.45,
            "ethics_commonsense - EM (Robustness)": 0.375,
            "ethics_commonsense - EM (Fairness)": 0.4166666666666667,
            "ethics_virtue - EM": 0.8416666666666667,
            "ethics_virtue - EM (Robustness)": 0.775,
            "ethics_virtue - EM (Fairness)": 0.7666666666666667,
            "ethics_deontology - EM": 0.6916666666666667,
            "ethics_deontology - EM (Robustness)": 0.5333333333333333,
            "ethics_deontology - EM (Fairness)": 0.55,
            "ethics_utilitarianism - EM": 0.675,
            "ethics_utilitarianism - EM (Robustness)": 0.5166666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.5666666666666667,
            "ethics Mean Win Rate": 0.576797385620915,
            "Score": 0.3660284384396852
        }
    ],
    [
        "armelrandy_submission_a100_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.06273039907551418,
            "sam_sum - Stereotypes (race)": 0.5741496598639454,
            "sam_sum - Stereotypes (gender)": 0.2949482822595801,
            "sam_sum - Representation (race)": 0.2683982683982684,
            "sam_sum - Representation (gender)": 0.1465566306203756,
            "sam_sum Mean Win Rate": 0.6235294117647059,
            "corr2cause - EM": 0.5025,
            "corr2cause Mean Win Rate": 0.5808823529411765,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.272108843537415,
            "MATH Mean Win Rate": 0.9411764705882353,
            "ethics_justice - EM": 0.8083333333333333,
            "ethics_justice - EM (Robustness)": 0.7583333333333333,
            "ethics_justice - EM (Fairness)": 0.6916666666666667,
            "ethics_commonsense - EM": 0.43333333333333335,
            "ethics_commonsense - EM (Robustness)": 0.4,
            "ethics_commonsense - EM (Fairness)": 0.4,
            "ethics_virtue - EM": 0.8583333333333333,
            "ethics_virtue - EM (Robustness)": 0.775,
            "ethics_virtue - EM (Fairness)": 0.7833333333333333,
            "ethics_deontology - EM": 0.6666666666666666,
            "ethics_deontology - EM (Robustness)": 0.5416666666666666,
            "ethics_deontology - EM (Fairness)": 0.5416666666666666,
            "ethics_utilitarianism - EM": 0.6833333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.55,
            "ethics_utilitarianism - EM (Fairness)": 0.6,
            "ethics Mean Win Rate": 0.5844771241830066,
            "Score": 0.35099620493058453
        }
    ],
    [
        "armelrandy_submission_a100_3_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.03949141764995878,
            "sam_sum - Stereotypes (race)": 0.6,
            "sam_sum - Stereotypes (gender)": 0.15339895912191043,
            "sam_sum - Representation (race)": 0.36666666666666664,
            "sam_sum - Representation (gender)": 0.09552167698904238,
            "sam_sum Mean Win Rate": 0.5647058823529412,
            "corr2cause - EM": 0.5025,
            "corr2cause Mean Win Rate": 0.5808823529411765,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.29128014842300554,
            "MATH Mean Win Rate": 0.9705882352941176,
            "ethics_justice - EM": 0.8083333333333333,
            "ethics_justice - EM (Robustness)": 0.7583333333333333,
            "ethics_justice - EM (Fairness)": 0.7,
            "ethics_commonsense - EM": 0.43333333333333335,
            "ethics_commonsense - EM (Robustness)": 0.4166666666666667,
            "ethics_commonsense - EM (Fairness)": 0.43333333333333335,
            "ethics_virtue - EM": 0.8166666666666667,
            "ethics_virtue - EM (Robustness)": 0.7666666666666667,
            "ethics_virtue - EM (Fairness)": 0.7583333333333333,
            "ethics_deontology - EM": 0.6833333333333333,
            "ethics_deontology - EM (Robustness)": 0.55,
            "ethics_deontology - EM (Fairness)": 0.55,
            "ethics_utilitarianism - EM": 0.675,
            "ethics_utilitarianism - EM (Robustness)": 0.475,
            "ethics_utilitarianism - EM (Fairness)": 0.55,
            "ethics Mean Win Rate": 0.5392156862745098,
            "Score": 0.34069619761179354
        }
    ],
    [
        "datta0_neurips_submission_3_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.12786369101454875,
            "CNN/DailyMail - Stereotypes (race)": 0.6519607843137255,
            "CNN/DailyMail - Stereotypes (gender)": 0.38015833845821007,
            "CNN/DailyMail - Representation (race)": 0.3500837520938024,
            "CNN/DailyMail - Representation (gender)": 0.13809523809523813,
            "CNN/DailyMail Mean Win Rate": 0.7235294117647059,
            "sam_sum - ROUGE-2": null,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": null,
            "sam_sum - Representation (race)": null,
            "sam_sum - Representation (gender)": null,
            "sam_sum Mean Win Rate": 0.02281045751633987,
            "corr2cause - EM": 0.4975,
            "corr2cause Mean Win Rate": 0.4338235294117647,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.24628942486085342,
            "MATH Mean Win Rate": 0.8823529411764706,
            "ethics_justice - EM": 0.75,
            "ethics_justice - EM (Robustness)": 0.6833333333333333,
            "ethics_justice - EM (Fairness)": 0.5916666666666667,
            "ethics_commonsense - EM": 0.5083333333333333,
            "ethics_commonsense - EM (Robustness)": 0.4083333333333333,
            "ethics_commonsense - EM (Fairness)": 0.44166666666666665,
            "ethics_virtue - EM": 0.8166666666666667,
            "ethics_virtue - EM (Robustness)": 0.7,
            "ethics_virtue - EM (Fairness)": 0.7666666666666667,
            "ethics_deontology - EM": 0.7583333333333333,
            "ethics_deontology - EM (Robustness)": 0.625,
            "ethics_deontology - EM (Fairness)": 0.5666666666666667,
            "ethics_utilitarianism - EM": 0.6083333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.425,
            "ethics_utilitarianism - EM (Fairness)": 0.5,
            "ethics Mean Win Rate": 0.5369281045751634,
            "Score": 0.3206951210911184
        }
    ],
    [
        "royson_neurips_llm_efficiency_challenge_submission_a100_submission_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.09748090670003871,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.3410442786740364,
            "CNN/DailyMail - Representation (race)": 0.34234234234234234,
            "CNN/DailyMail - Representation (gender)": 0.058252427184465994,
            "CNN/DailyMail Mean Win Rate": 0.6405882352941177,
            "sam_sum - ROUGE-2": 0.059187631778586096,
            "sam_sum - Stereotypes (race)": 0.5833333333333333,
            "sam_sum - Stereotypes (gender)": 0.2896637240754888,
            "sam_sum - Representation (race)": 0.3506044905008636,
            "sam_sum - Representation (gender)": 0.019729425028184866,
            "sam_sum Mean Win Rate": 0.6647058823529411,
            "corr2cause - EM": 0.455,
            "corr2cause Mean Win Rate": 0.20588235294117646,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.052411873840445274,
            "MATH Mean Win Rate": 0.14705882352941177,
            "ethics_justice - EM": 0.5666666666666667,
            "ethics_justice - EM (Robustness)": 0.5333333333333333,
            "ethics_justice - EM (Fairness)": 0.5583333333333333,
            "ethics_commonsense - EM": 0.49166666666666664,
            "ethics_commonsense - EM (Robustness)": 0.4083333333333333,
            "ethics_commonsense - EM (Fairness)": 0.4166666666666667,
            "ethics_virtue - EM": 0.19166666666666668,
            "ethics_virtue - EM (Robustness)": 0.19166666666666668,
            "ethics_virtue - EM (Fairness)": 0.19166666666666668,
            "ethics_deontology - EM": 0.575,
            "ethics_deontology - EM (Robustness)": 0.5666666666666667,
            "ethics_deontology - EM (Fairness)": 0.55,
            "ethics_utilitarianism - EM": 0.575,
            "ethics_utilitarianism - EM (Robustness)": 0.44166666666666665,
            "ethics_utilitarianism - EM (Fairness)": 0.5,
            "ethics Mean Win Rate": 0.23633986928104575,
            "Score": 0.31388541507645223
        }
    ],
    [
        "royson_neurips_llm_efficiency_challenge_submission_a100_submission_3_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.10855733721882721,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666669,
            "CNN/DailyMail - Stereotypes (gender)": 0.3046588554120836,
            "CNN/DailyMail - Representation (race)": 0.18974358974358974,
            "CNN/DailyMail - Representation (gender)": 0.007418397626112738,
            "CNN/DailyMail Mean Win Rate": 0.711764705882353,
            "sam_sum - ROUGE-2": 0.056751508913962356,
            "sam_sum - Stereotypes (race)": 0.5833333333333334,
            "sam_sum - Stereotypes (gender)": 0.34682740099406756,
            "sam_sum - Representation (race)": 0.37254901960784315,
            "sam_sum - Representation (gender)": 0.026867627785058967,
            "sam_sum Mean Win Rate": 0.5764705882352941,
            "corr2cause - EM": 0.45,
            "corr2cause Mean Win Rate": 0.17647058823529413,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.05225726654298083,
            "MATH Mean Win Rate": 0.11764705882352941,
            "ethics_justice - EM": 0.6333333333333333,
            "ethics_justice - EM (Robustness)": 0.5916666666666667,
            "ethics_justice - EM (Fairness)": 0.5583333333333333,
            "ethics_commonsense - EM": 0.525,
            "ethics_commonsense - EM (Robustness)": 0.475,
            "ethics_commonsense - EM (Fairness)": 0.49166666666666664,
            "ethics_virtue - EM": 0.19166666666666668,
            "ethics_virtue - EM (Robustness)": 0.19166666666666668,
            "ethics_virtue - EM (Fairness)": 0.19166666666666668,
            "ethics_deontology - EM": 0.5666666666666667,
            "ethics_deontology - EM (Robustness)": 0.5666666666666667,
            "ethics_deontology - EM (Fairness)": 0.5583333333333333,
            "ethics_utilitarianism - EM": 0.6083333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.5083333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.525,
            "ethics Mean Win Rate": 0.34490196078431373,
            "Score": 0.3116108240648959
        }
    ],
    [
        "hqbbzsp_nips_submission_a100_submission_of_a100_submission_1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.17075295705801796,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3596208685494401,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.025782688766114198,
            "sam_sum Mean Win Rate": 0.6507843137254902,
            "corr2cause - EM": 0.4975,
            "corr2cause Mean Win Rate": 0.4338235294117647,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.08132343846629561,
            "MATH Mean Win Rate": 0.5,
            "ethics_justice - EM": 0.7333333333333333,
            "ethics_justice - EM (Robustness)": 0.6666666666666666,
            "ethics_justice - EM (Fairness)": 0.65,
            "ethics_commonsense - EM": 0.44166666666666665,
            "ethics_commonsense - EM (Robustness)": 0.35,
            "ethics_commonsense - EM (Fairness)": 0.4,
            "ethics_virtue - EM": 0.7916666666666666,
            "ethics_virtue - EM (Robustness)": 0.675,
            "ethics_virtue - EM (Fairness)": 0.6916666666666667,
            "ethics_deontology - EM": 0.7666666666666667,
            "ethics_deontology - EM (Robustness)": 0.6666666666666666,
            "ethics_deontology - EM (Fairness)": 0.6333333333333333,
            "ethics_utilitarianism - EM": 0.7,
            "ethics_utilitarianism - EM (Robustness)": 0.65,
            "ethics_utilitarianism - EM (Fairness)": 0.5583333333333333,
            "ethics Mean Win Rate": 0.5687908496732026,
            "Score": 0.2926578896416837
        }
    ],
    [
        "vinairesearch_vinai_neurips_llm_efficiency_submit_c_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1351428329350911,
            "CNN/DailyMail - Stereotypes (race)": 0.6565656565656567,
            "CNN/DailyMail - Stereotypes (gender)": 0.43895873571373845,
            "CNN/DailyMail - Representation (race)": 0.3939393939393939,
            "CNN/DailyMail - Representation (gender)": 0.2086466165413534,
            "CNN/DailyMail Mean Win Rate": 0.538235294117647,
            "sam_sum - ROUGE-2": 0.08570542077423365,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3361024751781055,
            "sam_sum - Representation (race)": 0.3532338308457711,
            "sam_sum - Representation (gender)": 0.044765840220385655,
            "sam_sum Mean Win Rate": 0.5225490196078432,
            "corr2cause - EM": 0.4825,
            "corr2cause Mean Win Rate": 0.3235294117647059,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.03246753246753247,
            "MATH Mean Win Rate": 0.029411764705882353,
            "ethics_justice - EM": 0.7583333333333333,
            "ethics_justice - EM (Robustness)": 0.675,
            "ethics_justice - EM (Fairness)": 0.6666666666666666,
            "ethics_commonsense - EM": 0.4666666666666667,
            "ethics_commonsense - EM (Robustness)": 0.3,
            "ethics_commonsense - EM (Fairness)": 0.3416666666666667,
            "ethics_virtue - EM": 0.7166666666666667,
            "ethics_virtue - EM (Robustness)": 0.5833333333333334,
            "ethics_virtue - EM (Fairness)": 0.6666666666666666,
            "ethics_deontology - EM": 0.6333333333333333,
            "ethics_deontology - EM (Robustness)": 0.5666666666666667,
            "ethics_deontology - EM (Fairness)": 0.5833333333333334,
            "ethics_utilitarianism - EM": 0.725,
            "ethics_utilitarianism - EM (Robustness)": 0.5416666666666666,
            "ethics_utilitarianism - EM (Fairness)": 0.625,
            "ethics Mean Win Rate": 0.49117647058823527,
            "Score": 0.2653104297148751
        }
    ],
    [
        "datta0_neurips_submission_1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.12281922958263389,
            "CNN/DailyMail - Stereotypes (race)": 0.6230936819172113,
            "CNN/DailyMail - Stereotypes (gender)": 0.3209541620066886,
            "CNN/DailyMail - Representation (race)": 0.47040498442367595,
            "CNN/DailyMail - Representation (gender)": 0.16706443914081148,
            "CNN/DailyMail Mean Win Rate": 0.6470588235294118,
            "sam_sum - ROUGE-2": null,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": null,
            "sam_sum - Representation (race)": null,
            "sam_sum - Representation (gender)": null,
            "sam_sum Mean Win Rate": 0.02281045751633987,
            "corr2cause - EM": 0.515,
            "corr2cause Mean Win Rate": 0.7941176470588235,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.06246134817563388,
            "MATH Mean Win Rate": 0.20588235294117646,
            "ethics_justice - EM": 0.6916666666666667,
            "ethics_justice - EM (Robustness)": 0.6583333333333333,
            "ethics_justice - EM (Fairness)": 0.5583333333333333,
            "ethics_commonsense - EM": 0.525,
            "ethics_commonsense - EM (Robustness)": 0.4,
            "ethics_commonsense - EM (Fairness)": 0.48333333333333334,
            "ethics_virtue - EM": 0.8416666666666667,
            "ethics_virtue - EM (Robustness)": 0.7666666666666667,
            "ethics_virtue - EM (Fairness)": 0.775,
            "ethics_deontology - EM": 0.7333333333333333,
            "ethics_deontology - EM (Robustness)": 0.575,
            "ethics_deontology - EM (Fairness)": 0.55,
            "ethics_utilitarianism - EM": 0.625,
            "ethics_utilitarianism - EM (Robustness)": 0.5083333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.49166666666666664,
            "ethics Mean Win Rate": 0.5067320261437909,
            "Score": 0.2615001947451686
        }
    ],
    [
        "gyuwon12_submission_v3_submission_v3_sample_submissions_llama_recipes_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.1268171096933329,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.3190399877899878,
            "sam_sum - Representation (race)": 0.3333333333333333,
            "sam_sum - Representation (gender)": 0.04026845637583898,
            "sam_sum Mean Win Rate": 0.5452614379084968,
            "corr2cause - EM": 0.4625,
            "corr2cause Mean Win Rate": 0.27941176470588236,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.07498453927025357,
            "MATH Mean Win Rate": 0.38235294117647056,
            "ethics_justice - EM": 0.8333333333333334,
            "ethics_justice - EM (Robustness)": 0.7333333333333333,
            "ethics_justice - EM (Fairness)": 0.75,
            "ethics_commonsense - EM": 0.43333333333333335,
            "ethics_commonsense - EM (Robustness)": 0.425,
            "ethics_commonsense - EM (Fairness)": 0.4,
            "ethics_virtue - EM": 0.19166666666666668,
            "ethics_virtue - EM (Robustness)": 0.19166666666666668,
            "ethics_virtue - EM (Fairness)": 0.19166666666666668,
            "ethics_deontology - EM": 0.5916666666666667,
            "ethics_deontology - EM (Robustness)": 0.5583333333333333,
            "ethics_deontology - EM (Fairness)": 0.5666666666666667,
            "ethics_utilitarianism - EM": 0.7416666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.6083333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.6666666666666666,
            "ethics Mean Win Rate": 0.5194117647058824,
            "Score": 0.2407635939829302
        }
    ],
    [
        "facico_nips_submit_1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.17526385158913715,
            "CNN/DailyMail - Stereotypes (race)": 0.5606469002695419,
            "CNN/DailyMail - Stereotypes (gender)": 0.09692969260021195,
            "CNN/DailyMail - Representation (race)": 0.2110453648915187,
            "CNN/DailyMail - Representation (gender)": 0.03418124006359302,
            "CNN/DailyMail Mean Win Rate": 0.9352941176470588,
            "sam_sum - ROUGE-2": 0.007068146808211647,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": null,
            "sam_sum - Representation (race)": 0.6666666666666667,
            "sam_sum - Representation (gender)": 0.033333333333333326,
            "sam_sum Mean Win Rate": 0.18526143790849675,
            "corr2cause - EM": 0.0,
            "corr2cause Mean Win Rate": 0.024509803921568624,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.06570810142238713,
            "MATH Mean Win Rate": 0.2647058823529412,
            "ethics_justice - EM": 0.7083333333333334,
            "ethics_justice - EM (Robustness)": 0.6416666666666667,
            "ethics_justice - EM (Fairness)": 0.625,
            "ethics_commonsense - EM": 0.475,
            "ethics_commonsense - EM (Robustness)": 0.375,
            "ethics_commonsense - EM (Fairness)": 0.45,
            "ethics_virtue - EM": 0.8833333333333333,
            "ethics_virtue - EM (Robustness)": 0.8666666666666667,
            "ethics_virtue - EM (Fairness)": 0.85,
            "ethics_deontology - EM": 0.7416666666666667,
            "ethics_deontology - EM (Robustness)": 0.5833333333333334,
            "ethics_deontology - EM (Fairness)": 0.5416666666666666,
            "ethics_utilitarianism - EM": 0.675,
            "ethics_utilitarianism - EM (Robustness)": 0.5333333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.5583333333333333,
            "ethics Mean Win Rate": 0.5845751633986929,
            "Score": 0.23095927335550173
        }
    ],
    [
        "gyuwon12_submission_v2_submission_v2_sample_submissions_llama_recipes_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.12461549414737151,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.3964285714285714,
            "sam_sum - Representation (race)": 0.38095238095238093,
            "sam_sum - Representation (gender)": 0.05706984667802387,
            "sam_sum Mean Win Rate": 0.3396732026143791,
            "corr2cause - EM": 0.4625,
            "corr2cause Mean Win Rate": 0.27941176470588236,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.0782312925170068,
            "MATH Mean Win Rate": 0.4411764705882353,
            "ethics_justice - EM": 0.8333333333333334,
            "ethics_justice - EM (Robustness)": 0.725,
            "ethics_justice - EM (Fairness)": 0.75,
            "ethics_commonsense - EM": 0.43333333333333335,
            "ethics_commonsense - EM (Robustness)": 0.425,
            "ethics_commonsense - EM (Fairness)": 0.4,
            "ethics_virtue - EM": 0.19166666666666668,
            "ethics_virtue - EM (Robustness)": 0.19166666666666668,
            "ethics_virtue - EM (Fairness)": 0.19166666666666668,
            "ethics_deontology - EM": 0.5916666666666667,
            "ethics_deontology - EM (Robustness)": 0.5583333333333333,
            "ethics_deontology - EM (Fairness)": 0.5666666666666667,
            "ethics_utilitarianism - EM": 0.7416666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.6083333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.6666666666666666,
            "ethics Mean Win Rate": 0.5164705882352941,
            "Score": 0.22512227126311699
        }
    ],
    [
        "facico_nips_submit_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.18271036229975426,
            "CNN/DailyMail - Stereotypes (race)": 0.598997493734336,
            "CNN/DailyMail - Stereotypes (gender)": 0.110648819806536,
            "CNN/DailyMail - Representation (race)": 0.29166666666666663,
            "CNN/DailyMail - Representation (gender)": 0.03553719008264458,
            "CNN/DailyMail Mean Win Rate": 0.9,
            "sam_sum - ROUGE-2": 0.008686515911823824,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.5,
            "sam_sum - Representation (race)": 0.6666666666666667,
            "sam_sum - Representation (gender)": 0.09999999999999998,
            "sam_sum Mean Win Rate": 0.14967320261437908,
            "corr2cause - EM": 0.0,
            "corr2cause Mean Win Rate": 0.024509803921568624,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.06586270871985157,
            "MATH Mean Win Rate": 0.29411764705882354,
            "ethics_justice - EM": 0.7083333333333334,
            "ethics_justice - EM (Robustness)": 0.6416666666666667,
            "ethics_justice - EM (Fairness)": 0.6333333333333333,
            "ethics_commonsense - EM": 0.35,
            "ethics_commonsense - EM (Robustness)": 0.30833333333333335,
            "ethics_commonsense - EM (Fairness)": 0.30833333333333335,
            "ethics_virtue - EM": 0.8833333333333333,
            "ethics_virtue - EM (Robustness)": 0.8666666666666667,
            "ethics_virtue - EM (Fairness)": 0.8416666666666667,
            "ethics_deontology - EM": 0.7,
            "ethics_deontology - EM (Robustness)": 0.6,
            "ethics_deontology - EM (Fairness)": 0.6333333333333333,
            "ethics_utilitarianism - EM": 0.525,
            "ethics_utilitarianism - EM (Robustness)": 0.39166666666666666,
            "ethics_utilitarianism - EM (Fairness)": 0.45,
            "ethics Mean Win Rate": 0.4376797385620915,
            "Score": 0.21168046683966335
        }
    ],
    [
        "facico_nips_submit_3_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1804559183316493,
            "CNN/DailyMail - Stereotypes (race)": 0.5905349794238685,
            "CNN/DailyMail - Stereotypes (gender)": 0.08471424202696498,
            "CNN/DailyMail - Representation (race)": 0.2549019607843137,
            "CNN/DailyMail - Representation (gender)": 0.0346869712351946,
            "CNN/DailyMail Mean Win Rate": 0.9235294117647058,
            "sam_sum - ROUGE-2": 0.008765880991188904,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.5,
            "sam_sum - Representation (race)": 0.6666666666666667,
            "sam_sum - Representation (gender)": 0.0945945945945946,
            "sam_sum Mean Win Rate": 0.1673202614379085,
            "corr2cause - EM": 0.0,
            "corr2cause Mean Win Rate": 0.024509803921568624,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.07204700061842917,
            "MATH Mean Win Rate": 0.35294117647058826,
            "ethics_justice - EM": 0.7083333333333334,
            "ethics_justice - EM (Robustness)": 0.6416666666666667,
            "ethics_justice - EM (Fairness)": 0.6333333333333333,
            "ethics_commonsense - EM": 0.0,
            "ethics_commonsense - EM (Robustness)": 0.0,
            "ethics_commonsense - EM (Fairness)": 0.0,
            "ethics_virtue - EM": 0.8833333333333333,
            "ethics_virtue - EM (Robustness)": 0.8666666666666667,
            "ethics_virtue - EM (Fairness)": 0.8416666666666667,
            "ethics_deontology - EM": 0.0,
            "ethics_deontology - EM (Robustness)": 0.0,
            "ethics_deontology - EM (Fairness)": 0.0,
            "ethics_utilitarianism - EM": 0.0,
            "ethics_utilitarianism - EM (Robustness)": 0.0,
            "ethics_utilitarianism - EM (Fairness)": 0.0,
            "ethics Mean Win Rate": 0.24568627450980393,
            "Score": 0.2010409210496007
        }
    ],
    [
        "zsw256_nips_challenge_submission_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.0,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.0,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": null,
            "sam_sum - Representation (race)": null,
            "sam_sum - Representation (gender)": null,
            "sam_sum Mean Win Rate": 0.02281045751633987,
            "corr2cause - EM": 0.5025,
            "corr2cause Mean Win Rate": 0.5808823529411765,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.08565244279529993,
            "MATH Mean Win Rate": 0.5294117647058824,
            "ethics_justice - EM": 0.7166666666666667,
            "ethics_justice - EM (Robustness)": 0.6666666666666666,
            "ethics_justice - EM (Fairness)": 0.55,
            "ethics_commonsense - EM": 0.5416666666666666,
            "ethics_commonsense - EM (Robustness)": 0.38333333333333336,
            "ethics_commonsense - EM (Fairness)": 0.48333333333333334,
            "ethics_virtue - EM": 0.8333333333333334,
            "ethics_virtue - EM (Robustness)": 0.75,
            "ethics_virtue - EM (Fairness)": 0.825,
            "ethics_deontology - EM": 0.6583333333333333,
            "ethics_deontology - EM (Robustness)": 0.5583333333333333,
            "ethics_deontology - EM (Fairness)": 0.5,
            "ethics_utilitarianism - EM": 0.6,
            "ethics_utilitarianism - EM (Robustness)": 0.5,
            "ethics_utilitarianism - EM (Fairness)": 0.5083333333333333,
            "ethics Mean Win Rate": 0.46137254901960784,
            "Score": 0.1539710896227203
        }
    ],
    [
        "ranchlai_a100_track_nips2023_final_submission1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.1720398366033118,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.3454415954415954,
            "sam_sum - Representation (race)": 0.38095238095238093,
            "sam_sum - Representation (gender)": 0.015544041450777202,
            "sam_sum Mean Win Rate": 0.5220261437908497,
            "corr2cause - EM": null,
            "corr2cause Mean Win Rate": 0.024509803921568624,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.09152752009894867,
            "MATH Mean Win Rate": 0.5588235294117647,
            "ethics_justice - EM": 0.5916666666666667,
            "ethics_justice - EM (Robustness)": 0.525,
            "ethics_justice - EM (Fairness)": 0.5416666666666666,
            "ethics_commonsense - EM": null,
            "ethics_commonsense - EM (Robustness)": null,
            "ethics_commonsense - EM (Fairness)": null,
            "ethics_virtue - EM": 0.6583333333333333,
            "ethics_virtue - EM (Robustness)": 0.6083333333333333,
            "ethics_virtue - EM (Fairness)": 0.55,
            "ethics_deontology - EM": 0.6333333333333333,
            "ethics_deontology - EM (Robustness)": 0.575,
            "ethics_deontology - EM (Fairness)": 0.5666666666666667,
            "ethics_utilitarianism - EM": 0.7166666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.6166666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.6333333333333333,
            "ethics Mean Win Rate": 0.31562091503267975,
            "Score": 0.14325843067347868
        }
    ],
    [
        "ranchlai_a100_track_nips2023_final_submission3_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.16162578879697426,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.36054421768707484,
            "sam_sum - Representation (race)": 0.4,
            "sam_sum - Representation (gender)": 0.07704918032786887,
            "sam_sum Mean Win Rate": 0.38202614379084965,
            "corr2cause - EM": null,
            "corr2cause Mean Win Rate": 0.024509803921568624,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.09214594928880644,
            "MATH Mean Win Rate": 0.5882352941176471,
            "ethics_justice - EM": 0.5916666666666667,
            "ethics_justice - EM (Robustness)": 0.5083333333333333,
            "ethics_justice - EM (Fairness)": 0.5416666666666666,
            "ethics_commonsense - EM": null,
            "ethics_commonsense - EM (Robustness)": null,
            "ethics_commonsense - EM (Fairness)": null,
            "ethics_virtue - EM": 0.7083333333333334,
            "ethics_virtue - EM (Robustness)": 0.4583333333333333,
            "ethics_virtue - EM (Fairness)": 0.55,
            "ethics_deontology - EM": 0.7,
            "ethics_deontology - EM (Robustness)": 0.5833333333333334,
            "ethics_deontology - EM (Fairness)": 0.6083333333333333,
            "ethics_utilitarianism - EM": 0.6666666666666666,
            "ethics_utilitarianism - EM (Robustness)": 0.475,
            "ethics_utilitarianism - EM (Fairness)": 0.5333333333333333,
            "ethics Mean Win Rate": 0.26781045751633986,
            "Score": 0.13157942550959917
        }
    ],
    [
        "ranchlai_a100_track_nips2023_final_submission2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.17573064596851112,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.3854166666666667,
            "sam_sum - Representation (race)": 0.4444444444444444,
            "sam_sum - Representation (gender)": 0.05635491606714629,
            "sam_sum Mean Win Rate": 0.38790849673202615,
            "corr2cause - EM": null,
            "corr2cause Mean Win Rate": 0.024509803921568624,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.04901051329622758,
            "MATH Mean Win Rate": 0.08823529411764706,
            "ethics_justice - EM": 0.7083333333333334,
            "ethics_justice - EM (Robustness)": 0.6416666666666667,
            "ethics_justice - EM (Fairness)": 0.6583333333333333,
            "ethics_commonsense - EM": null,
            "ethics_commonsense - EM (Robustness)": null,
            "ethics_commonsense - EM (Fairness)": null,
            "ethics_virtue - EM": 0.575,
            "ethics_virtue - EM (Robustness)": 0.5083333333333333,
            "ethics_virtue - EM (Fairness)": 0.43333333333333335,
            "ethics_deontology - EM": 0.6583333333333333,
            "ethics_deontology - EM (Robustness)": 0.6083333333333333,
            "ethics_deontology - EM (Fairness)": 0.6,
            "ethics_utilitarianism - EM": 0.7083333333333334,
            "ethics_utilitarianism - EM (Robustness)": 0.575,
            "ethics_utilitarianism - EM (Fairness)": 0.6166666666666667,
            "ethics Mean Win Rate": 0.40679738562091505,
            "Score": 0.09818465164538302
        }
    ],
    [
        "gromovandreymeta_neurips_sub2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.02673796791443851,
            "sam_sum - ROUGE-2": 0.09585918856618181,
            "sam_sum - Stereotypes (race)": 0.6666666666666666,
            "sam_sum - Stereotypes (gender)": 0.24537037037037035,
            "sam_sum - Representation (race)": 0.3333333333333333,
            "sam_sum - Representation (gender)": 0.0058823529411764774,
            "sam_sum Mean Win Rate": 0.7738235294117647,
            "corr2cause - EM": 0.505,
            "corr2cause Mean Win Rate": 0.6960784313725491,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.02303648732220161,
            "MATH Mean Win Rate": 0.0,
            "ethics_justice - EM": 0.8916666666666667,
            "ethics_justice - EM (Robustness)": 0.8416666666666667,
            "ethics_justice - EM (Fairness)": 0.7666666666666667,
            "ethics_commonsense - EM": 0.3333333333333333,
            "ethics_commonsense - EM (Robustness)": 0.3,
            "ethics_commonsense - EM (Fairness)": 0.275,
            "ethics_virtue - EM": 0.6583333333333333,
            "ethics_virtue - EM (Robustness)": 0.6083333333333333,
            "ethics_virtue - EM (Fairness)": 0.5916666666666667,
            "ethics_deontology - EM": 0.8583333333333333,
            "ethics_deontology - EM (Robustness)": 0.7583333333333333,
            "ethics_deontology - EM (Fairness)": 0.8166666666666667,
            "ethics_utilitarianism - EM": 0.8416666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.8083333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.7666666666666667,
            "ethics Mean Win Rate": 0.653921568627451,
            "Score": 8.58119488005024e-66
        }
    ]
]