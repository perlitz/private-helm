[
    [
        "akjindal53244_neurips_submission_submission_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.17865889160914947,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666669,
            "CNN/DailyMail - Stereotypes (gender)": 0.449704254079254,
            "CNN/DailyMail - Representation (race)": 0.4666666666666667,
            "CNN/DailyMail - Representation (gender)": 0.19463869463869465,
            "CNN/DailyMail Mean Win Rate": 0.3793548387096774,
            "sam_sum - ROUGE-2": 0.16679099086590465,
            "sam_sum - Stereotypes (race)": 0.6666666666666666,
            "sam_sum - Stereotypes (gender)": 0.20263157894736844,
            "sam_sum - Representation (race)": 0.4,
            "sam_sum - Representation (gender)": 0.014820592823712925,
            "sam_sum Mean Win Rate": 0.7462365591397849,
            "corr2cause - EM": 0.5375,
            "corr2cause Mean Win Rate": 0.9032258064516129,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.12724180581323438,
            "MATH Mean Win Rate": 0.8064516129032258,
            "ethics_justice - EM": 0.7416666666666667,
            "ethics_justice - EM (Robustness)": 0.7,
            "ethics_justice - EM (Fairness)": 0.6666666666666666,
            "ethics_commonsense - EM": 0.43333333333333335,
            "ethics_commonsense - EM (Robustness)": 0.375,
            "ethics_commonsense - EM (Fairness)": 0.38333333333333336,
            "ethics_virtue - EM": 0.8583333333333333,
            "ethics_virtue - EM (Robustness)": 0.7916666666666666,
            "ethics_virtue - EM (Fairness)": 0.8,
            "ethics_deontology - EM": 0.675,
            "ethics_deontology - EM (Robustness)": 0.5666666666666667,
            "ethics_deontology - EM (Fairness)": 0.6333333333333333,
            "ethics_utilitarianism - EM": 0.7333333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.6166666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.65,
            "ethics Mean Win Rate": 0.685284178187404,
            "Score": 0.6761351303605458
        }
    ],
    [
        "quyanh2005_neurips_llm_challenge_4090_mistral_7b_neurips_v2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.13921575518155213,
            "CNN/DailyMail - Stereotypes (race)": 0.6428571428571429,
            "CNN/DailyMail - Stereotypes (gender)": 0.33689630413159827,
            "CNN/DailyMail - Representation (race)": 0.4715447154471545,
            "CNN/DailyMail - Representation (gender)": 0.15931372549019607,
            "CNN/DailyMail Mean Win Rate": 0.6129032258064516,
            "sam_sum - ROUGE-2": 0.09636561157098184,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3364417606122909,
            "sam_sum - Representation (race)": 0.35,
            "sam_sum - Representation (gender)": 0.0038979447200566675,
            "sam_sum Mean Win Rate": 0.6253456221198157,
            "corr2cause - EM": 0.54,
            "corr2cause Mean Win Rate": 0.9354838709677419,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.061997526283240574,
            "MATH Mean Win Rate": 0.5806451612903226,
            "ethics_justice - EM": 0.7333333333333333,
            "ethics_justice - EM (Robustness)": 0.6833333333333333,
            "ethics_justice - EM (Fairness)": 0.6833333333333333,
            "ethics_commonsense - EM": 0.525,
            "ethics_commonsense - EM (Robustness)": 0.4083333333333333,
            "ethics_commonsense - EM (Fairness)": 0.45,
            "ethics_virtue - EM": 0.825,
            "ethics_virtue - EM (Robustness)": 0.7833333333333333,
            "ethics_virtue - EM (Fairness)": 0.7666666666666667,
            "ethics_deontology - EM": 0.6666666666666666,
            "ethics_deontology - EM (Robustness)": 0.5083333333333333,
            "ethics_deontology - EM (Fairness)": 0.6,
            "ethics_utilitarianism - EM": 0.6083333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.475,
            "ethics_utilitarianism - EM (Fairness)": 0.5416666666666666,
            "ethics Mean Win Rate": 0.6082795698924731,
            "Score": 0.6614735352680238
        }
    ],
    [
        "ycchen_tw_neurips_llm_efficiency_challenge_4090_submissions_qwen_8bit_pure_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1631685101589,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.44444914060298674,
            "CNN/DailyMail - Representation (race)": 0.49743589743589745,
            "CNN/DailyMail - Representation (gender)": 0.19021739130434784,
            "CNN/DailyMail Mean Win Rate": 0.39838709677419354,
            "sam_sum - ROUGE-2": 0.15259831428330276,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3315734989648033,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.003477051460361591,
            "sam_sum Mean Win Rate": 0.6826036866359446,
            "corr2cause - EM": 0.5175,
            "corr2cause Mean Win Rate": 0.8387096774193549,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.3212739641311069,
            "MATH Mean Win Rate": 0.967741935483871,
            "ethics_justice - EM": 0.725,
            "ethics_justice - EM (Robustness)": 0.6583333333333333,
            "ethics_justice - EM (Fairness)": 0.5333333333333333,
            "ethics_commonsense - EM": 0.525,
            "ethics_commonsense - EM (Robustness)": 0.38333333333333336,
            "ethics_commonsense - EM (Fairness)": 0.48333333333333334,
            "ethics_virtue - EM": 0.875,
            "ethics_virtue - EM (Robustness)": 0.775,
            "ethics_virtue - EM (Fairness)": 0.7916666666666666,
            "ethics_deontology - EM": 0.675,
            "ethics_deontology - EM (Robustness)": 0.55,
            "ethics_deontology - EM (Fairness)": 0.48333333333333334,
            "ethics_utilitarianism - EM": 0.6,
            "ethics_utilitarianism - EM (Robustness)": 0.48333333333333334,
            "ethics_utilitarianism - EM (Fairness)": 0.5083333333333333,
            "ethics Mean Win Rate": 0.5091909882232463,
            "Score": 0.6458700757589597
        }
    ],
    [
        "agoncharenko1992_llm_eff_challenge_mistral_mistral_eval_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1336527158709118,
            "CNN/DailyMail - Stereotypes (race)": 0.6524822695035462,
            "CNN/DailyMail - Stereotypes (gender)": 0.42207497911445285,
            "CNN/DailyMail - Representation (race)": 0.39149888143176736,
            "CNN/DailyMail - Representation (gender)": 0.22459893048128338,
            "CNN/DailyMail Mean Win Rate": 0.45161290322580644,
            "sam_sum - ROUGE-2": 0.09369248585324778,
            "sam_sum - Stereotypes (race)": 0.6666666666666666,
            "sam_sum - Stereotypes (gender)": 0.3579809433330979,
            "sam_sum - Representation (race)": 0.3333333333333333,
            "sam_sum - Representation (gender)": 0.009185548071034888,
            "sam_sum Mean Win Rate": 0.7059139784946237,
            "corr2cause - EM": 0.4925,
            "corr2cause Mean Win Rate": 0.6451612903225806,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.09786641929499071,
            "MATH Mean Win Rate": 0.7741935483870968,
            "ethics_justice - EM": 0.7166666666666667,
            "ethics_justice - EM (Robustness)": 0.6,
            "ethics_justice - EM (Fairness)": 0.6083333333333333,
            "ethics_commonsense - EM": 0.45,
            "ethics_commonsense - EM (Robustness)": 0.325,
            "ethics_commonsense - EM (Fairness)": 0.39166666666666666,
            "ethics_virtue - EM": 0.8083333333333333,
            "ethics_virtue - EM (Robustness)": 0.7333333333333333,
            "ethics_virtue - EM (Fairness)": 0.7416666666666667,
            "ethics_deontology - EM": 0.6666666666666666,
            "ethics_deontology - EM (Robustness)": 0.6083333333333333,
            "ethics_deontology - EM (Fairness)": 0.6166666666666667,
            "ethics_utilitarianism - EM": 0.75,
            "ethics_utilitarianism - EM (Robustness)": 0.6416666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.675,
            "ethics Mean Win Rate": 0.5603942652329749,
            "Score": 0.6167454172688739
        }
    ],
    [
        "ycchen_tw_neurips_llm_efficiency_challenge_4090_submissions_qwen_8bit_qlora_arc_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.14750510574645298,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666669,
            "CNN/DailyMail - Stereotypes (gender)": 0.4254962690817954,
            "CNN/DailyMail - Representation (race)": 0.39809523809523806,
            "CNN/DailyMail - Representation (gender)": 0.21225577264653644,
            "CNN/DailyMail Mean Win Rate": 0.42129032258064514,
            "sam_sum - ROUGE-2": 0.08007746409728249,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.34324330230558125,
            "sam_sum - Representation (race)": 0.3333333333333333,
            "sam_sum - Representation (gender)": 0.010738255033556993,
            "sam_sum Mean Win Rate": 0.4817617866004963,
            "corr2cause - EM": 0.5425,
            "corr2cause Mean Win Rate": 0.967741935483871,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.26561533704390844,
            "MATH Mean Win Rate": 0.9032258064516129,
            "ethics_justice - EM": 0.7416666666666667,
            "ethics_justice - EM (Robustness)": 0.6833333333333333,
            "ethics_justice - EM (Fairness)": 0.6166666666666667,
            "ethics_commonsense - EM": 0.475,
            "ethics_commonsense - EM (Robustness)": 0.38333333333333336,
            "ethics_commonsense - EM (Fairness)": 0.38333333333333336,
            "ethics_virtue - EM": 0.8083333333333333,
            "ethics_virtue - EM (Robustness)": 0.7583333333333333,
            "ethics_virtue - EM (Fairness)": 0.725,
            "ethics_deontology - EM": 0.5583333333333333,
            "ethics_deontology - EM (Robustness)": 0.4583333333333333,
            "ethics_deontology - EM (Fairness)": 0.4666666666666667,
            "ethics_utilitarianism - EM": 0.625,
            "ethics_utilitarianism - EM (Robustness)": 0.5083333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.49166666666666664,
            "ethics Mean Win Rate": 0.4217921146953405,
            "Score": 0.5954065902732104
        }
    ],
    [
        "ycchen_tw_neurips_llm_efficiency_challenge_4090_submissions_qwen_8bit_qlora_r64_ep5_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.16399140904985265,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666669,
            "CNN/DailyMail - Stereotypes (gender)": 0.4221713471713472,
            "CNN/DailyMail - Representation (race)": 0.4042553191489362,
            "CNN/DailyMail - Representation (gender)": 0.21563981042654026,
            "CNN/DailyMail Mean Win Rate": 0.4374193548387097,
            "sam_sum - ROUGE-2": 0.16873556839666187,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3900191655609612,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.016212710765239974,
            "sam_sum Mean Win Rate": 0.540668202764977,
            "corr2cause - EM": 0.4875,
            "corr2cause Mean Win Rate": 0.6129032258064516,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.2818491032776747,
            "MATH Mean Win Rate": 0.9354838709677419,
            "ethics_justice - EM": 0.6916666666666667,
            "ethics_justice - EM (Robustness)": 0.65,
            "ethics_justice - EM (Fairness)": 0.5666666666666667,
            "ethics_commonsense - EM": 0.49166666666666664,
            "ethics_commonsense - EM (Robustness)": 0.4,
            "ethics_commonsense - EM (Fairness)": 0.43333333333333335,
            "ethics_virtue - EM": 0.8,
            "ethics_virtue - EM (Robustness)": 0.775,
            "ethics_virtue - EM (Fairness)": 0.7083333333333334,
            "ethics_deontology - EM": 0.675,
            "ethics_deontology - EM (Robustness)": 0.55,
            "ethics_deontology - EM (Fairness)": 0.5166666666666667,
            "ethics_utilitarianism - EM": 0.6916666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.5083333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.5416666666666666,
            "ethics Mean Win Rate": 0.503205325140809,
            "Score": 0.584521449860351
        }
    ],
    [
        "royson_neurips_llm_efficiency_challenge_submission_4090_4090_submission_1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.11666524287213376,
            "CNN/DailyMail - Stereotypes (race)": 0.6190476190476191,
            "CNN/DailyMail - Stereotypes (gender)": 0.3837982906801052,
            "CNN/DailyMail - Representation (race)": 0.3706666666666667,
            "CNN/DailyMail - Representation (gender)": 0.12822719449225473,
            "CNN/DailyMail Mean Win Rate": 0.6516129032258065,
            "sam_sum - ROUGE-2": 0.057499734683414454,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.32977357609710556,
            "sam_sum - Representation (race)": 0.3178294573643411,
            "sam_sum - Representation (gender)": 0.009056244041944717,
            "sam_sum Mean Win Rate": 0.5349875930521092,
            "corr2cause - EM": 0.4975,
            "corr2cause Mean Win Rate": 0.6935483870967742,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.07189239332096475,
            "MATH Mean Win Rate": 0.6451612903225806,
            "ethics_justice - EM": 0.575,
            "ethics_justice - EM (Robustness)": 0.55,
            "ethics_justice - EM (Fairness)": 0.5583333333333333,
            "ethics_commonsense - EM": 0.55,
            "ethics_commonsense - EM (Robustness)": 0.48333333333333334,
            "ethics_commonsense - EM (Fairness)": 0.49166666666666664,
            "ethics_virtue - EM": 0.19166666666666668,
            "ethics_virtue - EM (Robustness)": 0.19166666666666668,
            "ethics_virtue - EM (Fairness)": 0.19166666666666668,
            "ethics_deontology - EM": 0.6166666666666667,
            "ethics_deontology - EM (Robustness)": 0.575,
            "ethics_deontology - EM (Fairness)": 0.5833333333333334,
            "ethics_utilitarianism - EM": 0.5583333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.325,
            "ethics_utilitarianism - EM (Fairness)": 0.43333333333333335,
            "ethics Mean Win Rate": 0.3263440860215054,
            "Score": 0.5512528402882294
        }
    ],
    [
        "royson_neurips_llm_efficiency_challenge_submission_4090_4090_submission_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1104483533845868,
            "CNN/DailyMail - Stereotypes (race)": 0.638888888888889,
            "CNN/DailyMail - Stereotypes (gender)": 0.3374242407229292,
            "CNN/DailyMail - Representation (race)": 0.45945945945945943,
            "CNN/DailyMail - Representation (gender)": 0.11589403973509937,
            "CNN/DailyMail Mean Win Rate": 0.5612903225806452,
            "sam_sum - ROUGE-2": 0.04827618621499091,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.3185401157981803,
            "sam_sum - Representation (race)": 0.2897822445561139,
            "sam_sum - Representation (gender)": 0.012618296529968487,
            "sam_sum Mean Win Rate": 0.5156327543424318,
            "corr2cause - EM": 0.505,
            "corr2cause Mean Win Rate": 0.8064516129032258,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.05534941249226964,
            "MATH Mean Win Rate": 0.5161290322580645,
            "ethics_justice - EM": 0.675,
            "ethics_justice - EM (Robustness)": 0.6333333333333333,
            "ethics_justice - EM (Fairness)": 0.575,
            "ethics_commonsense - EM": 0.475,
            "ethics_commonsense - EM (Robustness)": 0.36666666666666664,
            "ethics_commonsense - EM (Fairness)": 0.4,
            "ethics_virtue - EM": 0.49166666666666664,
            "ethics_virtue - EM (Robustness)": 0.39166666666666666,
            "ethics_virtue - EM (Fairness)": 0.43333333333333335,
            "ethics_deontology - EM": 0.7083333333333334,
            "ethics_deontology - EM (Robustness)": 0.5333333333333333,
            "ethics_deontology - EM (Fairness)": 0.6083333333333333,
            "ethics_utilitarianism - EM": 0.5,
            "ethics_utilitarianism - EM (Robustness)": 0.25833333333333336,
            "ethics_utilitarianism - EM (Fairness)": 0.39166666666666666,
            "ethics Mean Win Rate": 0.25483870967741934,
            "Score": 0.49822556751775
        }
    ],
    [
        "mrigankraman_llm_comp_4090_submissions_4090_1st_submission_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.14930294115926734,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666666,
            "CNN/DailyMail - Stereotypes (gender)": 0.4279354904354904,
            "CNN/DailyMail - Representation (race)": 0.47517730496453897,
            "CNN/DailyMail - Representation (gender)": 0.19172932330827067,
            "CNN/DailyMail Mean Win Rate": 0.42473118279569894,
            "sam_sum - ROUGE-2": 0.08209239947890966,
            "sam_sum - Stereotypes (race)": 0.6666666666666666,
            "sam_sum - Stereotypes (gender)": 0.34594155844155833,
            "sam_sum - Representation (race)": 0.3333333333333333,
            "sam_sum - Representation (gender)": 0.015695067264573953,
            "sam_sum Mean Win Rate": 0.6478494623655914,
            "corr2cause - EM": 0.48,
            "corr2cause Mean Win Rate": 0.5376344086021506,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.052411873840445274,
            "MATH Mean Win Rate": 0.45161290322580644,
            "ethics_justice - EM": 0.7333333333333333,
            "ethics_justice - EM (Robustness)": 0.7083333333333334,
            "ethics_justice - EM (Fairness)": 0.625,
            "ethics_commonsense - EM": 0.55,
            "ethics_commonsense - EM (Robustness)": 0.4666666666666667,
            "ethics_commonsense - EM (Fairness)": 0.525,
            "ethics_virtue - EM": 0.7166666666666667,
            "ethics_virtue - EM (Robustness)": 0.675,
            "ethics_virtue - EM (Fairness)": 0.65,
            "ethics_deontology - EM": 0.6083333333333333,
            "ethics_deontology - EM (Robustness)": 0.5416666666666666,
            "ethics_deontology - EM (Fairness)": 0.5333333333333333,
            "ethics_utilitarianism - EM": 0.48333333333333334,
            "ethics_utilitarianism - EM (Robustness)": 0.31666666666666665,
            "ethics_utilitarianism - EM (Fairness)": 0.375,
            "ethics Mean Win Rate": 0.4403225806451613,
            "Score": 0.4939949633492069
        }
    ],
    [
        "hfvienna_unimportant_submission_4090_3_fine_tune_2_submission_4090_3_fine_tune_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.12659714811732403,
            "CNN/DailyMail - Stereotypes (race)": 0.6526610644257702,
            "CNN/DailyMail - Stereotypes (gender)": 0.41471306882179454,
            "CNN/DailyMail - Representation (race)": 0.36318407960199,
            "CNN/DailyMail - Representation (gender)": 0.21465968586387432,
            "CNN/DailyMail Mean Win Rate": 0.47096774193548385,
            "sam_sum - ROUGE-2": 0.08181111639891021,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.23881091011650554,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.017261556465769462,
            "sam_sum Mean Win Rate": 0.5019230769230769,
            "corr2cause - EM": 0.4975,
            "corr2cause Mean Win Rate": 0.6935483870967742,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.04236239950525665,
            "MATH Mean Win Rate": 0.3064516129032258,
            "ethics_justice - EM": 0.75,
            "ethics_justice - EM (Robustness)": 0.725,
            "ethics_justice - EM (Fairness)": 0.65,
            "ethics_commonsense - EM": 0.475,
            "ethics_commonsense - EM (Robustness)": 0.35,
            "ethics_commonsense - EM (Fairness)": 0.43333333333333335,
            "ethics_virtue - EM": 0.6583333333333333,
            "ethics_virtue - EM (Robustness)": 0.575,
            "ethics_virtue - EM (Fairness)": 0.575,
            "ethics_deontology - EM": 0.675,
            "ethics_deontology - EM (Robustness)": 0.5583333333333333,
            "ethics_deontology - EM (Fairness)": 0.6,
            "ethics_utilitarianism - EM": 0.6583333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.48333333333333334,
            "ethics_utilitarianism - EM (Fairness)": 0.5583333333333333,
            "ethics Mean Win Rate": 0.5299795186891961,
            "Score": 0.48424508111575876
        }
    ],
    [
        "mrigankraman_llm_comp_4090_submissions_4090_2nd_submission_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.14133859382148523,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.4263955342902711,
            "CNN/DailyMail - Representation (race)": 0.5666666666666667,
            "CNN/DailyMail - Representation (gender)": 0.16911764705882354,
            "CNN/DailyMail Mean Win Rate": 0.40483870967741936,
            "sam_sum - ROUGE-2": 0.08731586216745166,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.25535714285714284,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.030343007915567266,
            "sam_sum Mean Win Rate": 0.46966501240694786,
            "corr2cause - EM": 0.48,
            "corr2cause Mean Win Rate": 0.5376344086021506,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.055967841682127394,
            "MATH Mean Win Rate": 0.5483870967741935,
            "ethics_justice - EM": 0.7333333333333333,
            "ethics_justice - EM (Robustness)": 0.7083333333333334,
            "ethics_justice - EM (Fairness)": 0.6333333333333333,
            "ethics_commonsense - EM": 0.55,
            "ethics_commonsense - EM (Robustness)": 0.4583333333333333,
            "ethics_commonsense - EM (Fairness)": 0.525,
            "ethics_virtue - EM": 0.7166666666666667,
            "ethics_virtue - EM (Robustness)": 0.675,
            "ethics_virtue - EM (Fairness)": 0.65,
            "ethics_deontology - EM": 0.6083333333333333,
            "ethics_deontology - EM (Robustness)": 0.5416666666666666,
            "ethics_deontology - EM (Fairness)": 0.5416666666666666,
            "ethics_utilitarianism - EM": 0.48333333333333334,
            "ethics_utilitarianism - EM (Robustness)": 0.31666666666666665,
            "ethics_utilitarianism - EM (Fairness)": 0.375,
            "ethics Mean Win Rate": 0.4474910394265233,
            "Score": 0.4785042213773309
        }
    ],
    [
        "matthewdouglas_neurips_llm_challenge_2023_inference3_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.15687753584328307,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.42842857142857144,
            "CNN/DailyMail - Representation (race)": 0.4264432029795159,
            "CNN/DailyMail - Representation (gender)": 0.2032640949554896,
            "CNN/DailyMail Mean Win Rate": 0.4306451612903226,
            "sam_sum - ROUGE-2": 0.18631466590676107,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.30416666666666664,
            "sam_sum - Representation (race)": 0.38888888888888884,
            "sam_sum - Representation (gender)": 0.04643628509719222,
            "sam_sum Mean Win Rate": 0.4833746898263027,
            "corr2cause - EM": 0.4625,
            "corr2cause Mean Win Rate": 0.3225806451612903,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.19913419913419914,
            "MATH Mean Win Rate": 0.8387096774193549,
            "ethics_justice - EM": 0.7166666666666667,
            "ethics_justice - EM (Robustness)": 0.675,
            "ethics_justice - EM (Fairness)": 0.6416666666666667,
            "ethics_commonsense - EM": 0.5083333333333333,
            "ethics_commonsense - EM (Robustness)": 0.38333333333333336,
            "ethics_commonsense - EM (Fairness)": 0.44166666666666665,
            "ethics_virtue - EM": 0.7333333333333333,
            "ethics_virtue - EM (Robustness)": 0.625,
            "ethics_virtue - EM (Fairness)": 0.6833333333333333,
            "ethics_deontology - EM": 0.6333333333333333,
            "ethics_deontology - EM (Robustness)": 0.45,
            "ethics_deontology - EM (Fairness)": 0.48333333333333334,
            "ethics_utilitarianism - EM": 0.6083333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.49166666666666664,
            "ethics_utilitarianism - EM (Fairness)": 0.5166666666666667,
            "ethics Mean Win Rate": 0.410752688172043,
            "Score": 0.4708111837144683
        }
    ],
    [
        "quyanh2005_neurips_llm_challenge_4090_mistral_7b_neurips_v1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1371022984966275,
            "CNN/DailyMail - Stereotypes (race)": 0.6160714285714286,
            "CNN/DailyMail - Stereotypes (gender)": 0.39437214696658573,
            "CNN/DailyMail - Representation (race)": 0.43730886850152906,
            "CNN/DailyMail - Representation (gender)": 0.17567567567567569,
            "CNN/DailyMail Mean Win Rate": 0.6064516129032258,
            "sam_sum - ROUGE-2": 0.09098933541631016,
            "sam_sum - Stereotypes (race)": 0.6666666666666669,
            "sam_sum - Stereotypes (gender)": 0.40780255546647154,
            "sam_sum - Representation (race)": 0.3532338308457711,
            "sam_sum - Representation (gender)": 0.04051565377532229,
            "sam_sum Mean Win Rate": 0.36774193548387096,
            "corr2cause - EM": 0.5225,
            "corr2cause Mean Win Rate": 0.8709677419354839,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.029375386518243663,
            "MATH Mean Win Rate": 0.16129032258064516,
            "ethics_justice - EM": 0.725,
            "ethics_justice - EM (Robustness)": 0.7,
            "ethics_justice - EM (Fairness)": 0.6916666666666667,
            "ethics_commonsense - EM": 0.525,
            "ethics_commonsense - EM (Robustness)": 0.4,
            "ethics_commonsense - EM (Fairness)": 0.5,
            "ethics_virtue - EM": 0.7333333333333333,
            "ethics_virtue - EM (Robustness)": 0.6333333333333333,
            "ethics_virtue - EM (Fairness)": 0.65,
            "ethics_deontology - EM": 0.6583333333333333,
            "ethics_deontology - EM (Robustness)": 0.5666666666666667,
            "ethics_deontology - EM (Fairness)": 0.55,
            "ethics_utilitarianism - EM": 0.6083333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.4166666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.5416666666666666,
            "ethics Mean Win Rate": 0.5540501792114695,
            "Score": 0.44452865212502163
        }
    ],
    [
        "mrigankraman_llm_comp_4090_submissions_4090_3rd_submission_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1411125524597463,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666669,
            "CNN/DailyMail - Stereotypes (gender)": 0.4484377505210839,
            "CNN/DailyMail - Representation (race)": 0.4139194139194139,
            "CNN/DailyMail - Representation (gender)": 0.20783847980997625,
            "CNN/DailyMail Mean Win Rate": 0.3470967741935484,
            "sam_sum - ROUGE-2": 0.08844220827660905,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.28539473684210526,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.03033268101761255,
            "sam_sum Mean Win Rate": 0.4825682382133995,
            "corr2cause - EM": 0.465,
            "corr2cause Mean Win Rate": 0.3548387096774194,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.07498453927025354,
            "MATH Mean Win Rate": 0.7096774193548387,
            "ethics_justice - EM": 0.7083333333333334,
            "ethics_justice - EM (Robustness)": 0.6583333333333333,
            "ethics_justice - EM (Fairness)": 0.6166666666666667,
            "ethics_commonsense - EM": 0.5833333333333334,
            "ethics_commonsense - EM (Robustness)": 0.5083333333333333,
            "ethics_commonsense - EM (Fairness)": 0.5583333333333333,
            "ethics_virtue - EM": 0.6916666666666667,
            "ethics_virtue - EM (Robustness)": 0.6333333333333333,
            "ethics_virtue - EM (Fairness)": 0.625,
            "ethics_deontology - EM": 0.625,
            "ethics_deontology - EM (Robustness)": 0.5166666666666667,
            "ethics_deontology - EM (Fairness)": 0.5,
            "ethics_utilitarianism - EM": 0.49166666666666664,
            "ethics_utilitarianism - EM (Robustness)": 0.48333333333333334,
            "ethics_utilitarianism - EM (Fairness)": 0.36666666666666664,
            "ethics Mean Win Rate": 0.4103942652329749,
            "Score": 0.44428383772232594
        }
    ],
    [
        "hfvienna_unimportant_submission_4090_1_fine_tune_submission_4090_1_fine_tune_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1343967820288689,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666666,
            "CNN/DailyMail - Stereotypes (gender)": 0.4285270651889154,
            "CNN/DailyMail - Representation (race)": 0.4217687074829932,
            "CNN/DailyMail - Representation (gender)": 0.17191601049868765,
            "CNN/DailyMail Mean Win Rate": 0.44408602150537635,
            "sam_sum - ROUGE-2": 0.08141835200864697,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3750023356949607,
            "sam_sum - Representation (race)": 0.3615819209039548,
            "sam_sum - Representation (gender)": 0.03298909925415949,
            "sam_sum Mean Win Rate": 0.380184331797235,
            "corr2cause - EM": 0.5025,
            "corr2cause Mean Win Rate": 0.7580645161290323,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.032467532467532464,
            "MATH Mean Win Rate": 0.1935483870967742,
            "ethics_justice - EM": 0.7666666666666667,
            "ethics_justice - EM (Robustness)": 0.7166666666666667,
            "ethics_justice - EM (Fairness)": 0.6583333333333333,
            "ethics_commonsense - EM": 0.5,
            "ethics_commonsense - EM (Robustness)": 0.36666666666666664,
            "ethics_commonsense - EM (Fairness)": 0.45,
            "ethics_virtue - EM": 0.65,
            "ethics_virtue - EM (Robustness)": 0.575,
            "ethics_virtue - EM (Fairness)": 0.5666666666666667,
            "ethics_deontology - EM": 0.65,
            "ethics_deontology - EM (Robustness)": 0.5583333333333333,
            "ethics_deontology - EM (Fairness)": 0.575,
            "ethics_utilitarianism - EM": 0.6666666666666666,
            "ethics_utilitarianism - EM (Robustness)": 0.4666666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.5666666666666667,
            "ethics Mean Win Rate": 0.529247311827957,
            "Score": 0.4202649202575915
        }
    ],
    [
        "matthewdouglas_neurips_llm_challenge_2023_inference_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.16086744134058922,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666666,
            "CNN/DailyMail - Stereotypes (gender)": 0.43894467505567963,
            "CNN/DailyMail - Representation (race)": 0.4199134199134199,
            "CNN/DailyMail - Representation (gender)": 0.2098674521354934,
            "CNN/DailyMail Mean Win Rate": 0.44408602150537635,
            "sam_sum - ROUGE-2": 0.1805328051711478,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.33777777777777773,
            "sam_sum - Representation (race)": 0.40350877192982454,
            "sam_sum - Representation (gender)": 0.006550218340611369,
            "sam_sum Mean Win Rate": 0.5156327543424317,
            "corr2cause - EM": 0.4525,
            "corr2cause Mean Win Rate": 0.12903225806451613,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.21227581941867654,
            "MATH Mean Win Rate": 0.8709677419354839,
            "ethics_justice - EM": 0.7416666666666667,
            "ethics_justice - EM (Robustness)": 0.7,
            "ethics_justice - EM (Fairness)": 0.6583333333333333,
            "ethics_commonsense - EM": 0.525,
            "ethics_commonsense - EM (Robustness)": 0.35833333333333334,
            "ethics_commonsense - EM (Fairness)": 0.45,
            "ethics_virtue - EM": 0.6916666666666667,
            "ethics_virtue - EM (Robustness)": 0.625,
            "ethics_virtue - EM (Fairness)": 0.6166666666666667,
            "ethics_deontology - EM": 0.6583333333333333,
            "ethics_deontology - EM (Robustness)": 0.55,
            "ethics_deontology - EM (Fairness)": 0.575,
            "ethics_utilitarianism - EM": 0.6416666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.44166666666666665,
            "ethics_utilitarianism - EM (Fairness)": 0.5333333333333333,
            "ethics Mean Win Rate": 0.4968100358422939,
            "Score": 0.41815728338867425
        }
    ],
    [
        "tvergho_llm_neurips_train_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.14506926311317359,
            "CNN/DailyMail - Stereotypes (race)": 0.642857142857143,
            "CNN/DailyMail - Stereotypes (gender)": 0.45466515837104077,
            "CNN/DailyMail - Representation (race)": 0.4545454545454546,
            "CNN/DailyMail - Representation (gender)": 0.20791075050709937,
            "CNN/DailyMail Mean Win Rate": 0.4064516129032258,
            "sam_sum - ROUGE-2": 0.096074717913245,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.38051835317460314,
            "sam_sum - Representation (race)": 0.4523809523809524,
            "sam_sum - Representation (gender)": 0.02839756592292092,
            "sam_sum Mean Win Rate": 0.32853598014888336,
            "corr2cause - EM": 0.48,
            "corr2cause Mean Win Rate": 0.5376344086021506,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.05241187384044526,
            "MATH Mean Win Rate": 0.41935483870967744,
            "ethics_justice - EM": 0.7,
            "ethics_justice - EM (Robustness)": 0.6583333333333333,
            "ethics_justice - EM (Fairness)": 0.6666666666666666,
            "ethics_commonsense - EM": 0.4583333333333333,
            "ethics_commonsense - EM (Robustness)": 0.31666666666666665,
            "ethics_commonsense - EM (Fairness)": 0.39166666666666666,
            "ethics_virtue - EM": 0.8416666666666667,
            "ethics_virtue - EM (Robustness)": 0.7166666666666667,
            "ethics_virtue - EM (Fairness)": 0.7916666666666666,
            "ethics_deontology - EM": 0.5833333333333334,
            "ethics_deontology - EM (Robustness)": 0.5,
            "ethics_deontology - EM (Fairness)": 0.525,
            "ethics_utilitarianism - EM": 0.5833333333333334,
            "ethics_utilitarianism - EM (Robustness)": 0.5333333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.5583333333333333,
            "ethics Mean Win Rate": 0.4173835125448029,
            "Score": 0.41671502426912177
        }
    ],
    [
        "teja1729_llm_challenge_4090_track_submission_1_evaluation_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.12238346866455414,
            "CNN/DailyMail - Stereotypes (race)": 0.6333333333333333,
            "CNN/DailyMail - Stereotypes (gender)": 0.40037212370001235,
            "CNN/DailyMail - Representation (race)": 0.5279106858054227,
            "CNN/DailyMail - Representation (gender)": 0.17466266866566718,
            "CNN/DailyMail Mean Win Rate": 0.4645161290322581,
            "sam_sum - ROUGE-2": 0.06161341929635506,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3377743841810987,
            "sam_sum - Representation (race)": 0.33333333333333326,
            "sam_sum - Representation (gender)": 0.03404255319148933,
            "sam_sum Mean Win Rate": 0.5027649769585253,
            "corr2cause - EM": 0.46,
            "corr2cause Mean Win Rate": 0.2903225806451613,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.04251700680272109,
            "MATH Mean Win Rate": 0.3548387096774194,
            "ethics_justice - EM": 0.725,
            "ethics_justice - EM (Robustness)": 0.7,
            "ethics_justice - EM (Fairness)": 0.6583333333333333,
            "ethics_commonsense - EM": 0.5666666666666667,
            "ethics_commonsense - EM (Robustness)": 0.39166666666666666,
            "ethics_commonsense - EM (Fairness)": 0.475,
            "ethics_virtue - EM": 0.6416666666666667,
            "ethics_virtue - EM (Robustness)": 0.6083333333333333,
            "ethics_virtue - EM (Fairness)": 0.5583333333333333,
            "ethics_deontology - EM": 0.65,
            "ethics_deontology - EM (Robustness)": 0.5,
            "ethics_deontology - EM (Fairness)": 0.5333333333333333,
            "ethics_utilitarianism - EM": 0.5666666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.43333333333333335,
            "ethics_utilitarianism - EM (Fairness)": 0.49166666666666664,
            "ethics Mean Win Rate": 0.41093189964157706,
            "Score": 0.3972002338791995
        }
    ],
    [
        "shushengyuan_nips2023_challenge_submissions_submission1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.139580479334058,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666666,
            "CNN/DailyMail - Stereotypes (gender)": 0.3713409633551334,
            "CNN/DailyMail - Representation (race)": 0.3666666666666666,
            "CNN/DailyMail - Representation (gender)": 0.18990042674253202,
            "CNN/DailyMail Mean Win Rate": 0.624731182795699,
            "sam_sum - ROUGE-2": 0.09954775668472933,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.33718967986127996,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.027527527527527518,
            "sam_sum Mean Win Rate": 0.5600230414746543,
            "corr2cause - EM": 0.4775,
            "corr2cause Mean Win Rate": 0.4838709677419355,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.016233766233766236,
            "MATH Mean Win Rate": 0.0967741935483871,
            "ethics_justice - EM": 0.7333333333333333,
            "ethics_justice - EM (Robustness)": 0.7083333333333334,
            "ethics_justice - EM (Fairness)": 0.6666666666666666,
            "ethics_commonsense - EM": 0.4583333333333333,
            "ethics_commonsense - EM (Robustness)": 0.35833333333333334,
            "ethics_commonsense - EM (Fairness)": 0.4166666666666667,
            "ethics_virtue - EM": 0.7333333333333333,
            "ethics_virtue - EM (Robustness)": 0.675,
            "ethics_virtue - EM (Fairness)": 0.6416666666666667,
            "ethics_deontology - EM": 0.675,
            "ethics_deontology - EM (Robustness)": 0.5666666666666667,
            "ethics_deontology - EM (Fairness)": 0.575,
            "ethics_utilitarianism - EM": 0.6333333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.49166666666666664,
            "ethics_utilitarianism - EM (Fairness)": 0.525,
            "ethics Mean Win Rate": 0.5323092677931388,
            "Score": 0.387356142049056
        }
    ],
    [
        "shushengyuan_nips2023_challenge_submissions_submission3_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.13573363682749617,
            "CNN/DailyMail - Stereotypes (race)": 0.606060606060606,
            "CNN/DailyMail - Stereotypes (gender)": 0.37784894305203226,
            "CNN/DailyMail - Representation (race)": 0.4480620155038759,
            "CNN/DailyMail - Representation (gender)": 0.19852941176470587,
            "CNN/DailyMail Mean Win Rate": 0.5806451612903226,
            "sam_sum - ROUGE-2": 0.07115760077519256,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3680446537589394,
            "sam_sum - Representation (race)": 0.3333333333333333,
            "sam_sum - Representation (gender)": 0.001445086705202353,
            "sam_sum Mean Win Rate": 0.5850230414746543,
            "corr2cause - EM": 0.47,
            "corr2cause Mean Win Rate": 0.3870967741935484,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.016697588126159554,
            "MATH Mean Win Rate": 0.12903225806451613,
            "ethics_justice - EM": 0.7333333333333333,
            "ethics_justice - EM (Robustness)": 0.7166666666666667,
            "ethics_justice - EM (Fairness)": 0.7,
            "ethics_commonsense - EM": 0.425,
            "ethics_commonsense - EM (Robustness)": 0.2916666666666667,
            "ethics_commonsense - EM (Fairness)": 0.4,
            "ethics_virtue - EM": 0.675,
            "ethics_virtue - EM (Robustness)": 0.6083333333333333,
            "ethics_virtue - EM (Fairness)": 0.6083333333333333,
            "ethics_deontology - EM": 0.65,
            "ethics_deontology - EM (Robustness)": 0.5333333333333333,
            "ethics_deontology - EM (Fairness)": 0.5916666666666667,
            "ethics_utilitarianism - EM": 0.675,
            "ethics_utilitarianism - EM (Robustness)": 0.4666666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.5666666666666667,
            "ethics Mean Win Rate": 0.47519713261648744,
            "Score": 0.3813248330125892
        }
    ],
    [
        "teja1729_llm_challenge_4090_track_submission_2_evaluation_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.12689120320214553,
            "CNN/DailyMail - Stereotypes (race)": 0.6440422322775265,
            "CNN/DailyMail - Stereotypes (gender)": 0.3855812606350241,
            "CNN/DailyMail - Representation (race)": 0.29166666666666663,
            "CNN/DailyMail - Representation (gender)": 0.1909090909090909,
            "CNN/DailyMail Mean Win Rate": 0.6129032258064516,
            "sam_sum - ROUGE-2": 0.05722350565395237,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.32866217267306197,
            "sam_sum - Representation (race)": 0.4722222222222222,
            "sam_sum - Representation (gender)": 0.014629948364888123,
            "sam_sum Mean Win Rate": 0.4511520737327189,
            "corr2cause - EM": 0.455,
            "corr2cause Mean Win Rate": 0.1827956989247312,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.04236239950525665,
            "MATH Mean Win Rate": 0.3064516129032258,
            "ethics_justice - EM": 0.725,
            "ethics_justice - EM (Robustness)": 0.7,
            "ethics_justice - EM (Fairness)": 0.6416666666666667,
            "ethics_commonsense - EM": 0.5333333333333333,
            "ethics_commonsense - EM (Robustness)": 0.375,
            "ethics_commonsense - EM (Fairness)": 0.4583333333333333,
            "ethics_virtue - EM": 0.625,
            "ethics_virtue - EM (Robustness)": 0.5416666666666666,
            "ethics_virtue - EM (Fairness)": 0.5333333333333333,
            "ethics_deontology - EM": 0.675,
            "ethics_deontology - EM (Robustness)": 0.5583333333333333,
            "ethics_deontology - EM (Fairness)": 0.5666666666666667,
            "ethics_utilitarianism - EM": 0.6083333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.425,
            "ethics_utilitarianism - EM (Fairness)": 0.4666666666666667,
            "ethics Mean Win Rate": 0.40632360471070145,
            "Score": 0.36289665107220925
        }
    ],
    [
        "teja1729_llm_challenge_4090_track_submission_3_evaluation_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.12674819519114017,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666669,
            "CNN/DailyMail - Stereotypes (gender)": 0.3989989291725879,
            "CNN/DailyMail - Representation (race)": 0.39809523809523806,
            "CNN/DailyMail - Representation (gender)": 0.22739916550764944,
            "CNN/DailyMail Mean Win Rate": 0.3438709677419355,
            "sam_sum - ROUGE-2": 0.05405021391813687,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.40323928944618603,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.005617977528089901,
            "sam_sum Mean Win Rate": 0.45034562211981566,
            "corr2cause - EM": 0.455,
            "corr2cause Mean Win Rate": 0.1827956989247312,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.04576376004947434,
            "MATH Mean Win Rate": 0.3870967741935484,
            "ethics_justice - EM": 0.7416666666666667,
            "ethics_justice - EM (Robustness)": 0.7166666666666667,
            "ethics_justice - EM (Fairness)": 0.6666666666666666,
            "ethics_commonsense - EM": 0.5416666666666666,
            "ethics_commonsense - EM (Robustness)": 0.38333333333333336,
            "ethics_commonsense - EM (Fairness)": 0.4583333333333333,
            "ethics_virtue - EM": 0.625,
            "ethics_virtue - EM (Robustness)": 0.5416666666666666,
            "ethics_virtue - EM (Fairness)": 0.5333333333333333,
            "ethics_deontology - EM": 0.675,
            "ethics_deontology - EM (Robustness)": 0.5583333333333333,
            "ethics_deontology - EM (Fairness)": 0.5666666666666667,
            "ethics_utilitarianism - EM": 0.6166666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.43333333333333335,
            "ethics_utilitarianism - EM (Fairness)": 0.49166666666666664,
            "ethics Mean Win Rate": 0.49005120327700974,
            "Score": 0.3515553311856883
        }
    ],
    [
        "matthewdouglas_neurips_llm_challenge_2023_inference2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1354085541472297,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666667,
            "CNN/DailyMail - Stereotypes (gender)": 0.39652239866189376,
            "CNN/DailyMail - Representation (race)": 0.4066666666666666,
            "CNN/DailyMail - Representation (gender)": 0.22229822161422708,
            "CNN/DailyMail Mean Win Rate": 0.4112903225806452,
            "sam_sum - ROUGE-2": 0.13539779962017304,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3074386304909561,
            "sam_sum - Representation (race)": 0.47916666666666663,
            "sam_sum - Representation (gender)": 0.060585885486018676,
            "sam_sum Mean Win Rate": 0.4705069124423963,
            "corr2cause - EM": 0.455,
            "corr2cause Mean Win Rate": 0.18279569892473116,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.0391156462585034,
            "MATH Mean Win Rate": 0.25806451612903225,
            "ethics_justice - EM": 0.7416666666666667,
            "ethics_justice - EM (Robustness)": 0.725,
            "ethics_justice - EM (Fairness)": 0.65,
            "ethics_commonsense - EM": 0.55,
            "ethics_commonsense - EM (Robustness)": 0.375,
            "ethics_commonsense - EM (Fairness)": 0.4583333333333333,
            "ethics_virtue - EM": 0.6916666666666667,
            "ethics_virtue - EM (Robustness)": 0.6333333333333333,
            "ethics_virtue - EM (Fairness)": 0.6083333333333333,
            "ethics_deontology - EM": 0.6666666666666666,
            "ethics_deontology - EM (Robustness)": 0.5166666666666667,
            "ethics_deontology - EM (Fairness)": 0.5416666666666666,
            "ethics_utilitarianism - EM": 0.625,
            "ethics_utilitarianism - EM (Robustness)": 0.45,
            "ethics_utilitarianism - EM (Fairness)": 0.5166666666666667,
            "ethics Mean Win Rate": 0.5223297491039427,
            "Score": 0.34329755105510085
        }
    ],
    [
        "shushengyuan_nips2023_challenge_submissions_submission2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.12891833548122272,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666666,
            "CNN/DailyMail - Stereotypes (gender)": 0.40693731646912334,
            "CNN/DailyMail - Representation (race)": 0.44848484848484843,
            "CNN/DailyMail - Representation (gender)": 0.20721357850070724,
            "CNN/DailyMail Mean Win Rate": 0.3924731182795699,
            "sam_sum - ROUGE-2": 0.06573849607875404,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.36862026862026864,
            "sam_sum - Representation (race)": 0.34108527131782945,
            "sam_sum - Representation (gender)": 0.06316590563165905,
            "sam_sum Mean Win Rate": 0.33502304147465434,
            "corr2cause - EM": 0.4575,
            "corr2cause Mean Win Rate": 0.25806451612903225,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.03277674706246135,
            "MATH Mean Win Rate": 0.22580645161290322,
            "ethics_justice - EM": 0.75,
            "ethics_justice - EM (Robustness)": 0.7166666666666667,
            "ethics_justice - EM (Fairness)": 0.6833333333333333,
            "ethics_commonsense - EM": 0.5083333333333333,
            "ethics_commonsense - EM (Robustness)": 0.35,
            "ethics_commonsense - EM (Fairness)": 0.44166666666666665,
            "ethics_virtue - EM": 0.6666666666666666,
            "ethics_virtue - EM (Robustness)": 0.6083333333333333,
            "ethics_virtue - EM (Fairness)": 0.575,
            "ethics_deontology - EM": 0.6833333333333333,
            "ethics_deontology - EM (Robustness)": 0.5916666666666667,
            "ethics_deontology - EM (Fairness)": 0.5833333333333334,
            "ethics_utilitarianism - EM": 0.6333333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.43333333333333335,
            "ethics_utilitarianism - EM (Fairness)": 0.5,
            "ethics Mean Win Rate": 0.5481362007168459,
            "Score": 0.33469370452370834
        }
    ],
    [
        "chirnsch_llm_neurips_finetuning_submission_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1418202874777125,
            "CNN/DailyMail - Stereotypes (race)": 0.6574074074074073,
            "CNN/DailyMail - Stereotypes (gender)": 0.419722296380083,
            "CNN/DailyMail - Representation (race)": 0.42592592592592593,
            "CNN/DailyMail - Representation (gender)": 0.23229461756373943,
            "CNN/DailyMail Mean Win Rate": 0.43870967741935485,
            "sam_sum - ROUGE-2": 0.08771539577566866,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.310867130861548,
            "sam_sum - Representation (race)": 0.33333333333333337,
            "sam_sum - Representation (gender)": 0.046975546975546956,
            "sam_sum Mean Win Rate": 0.5084101382488478,
            "corr2cause - EM": 0.475,
            "corr2cause Mean Win Rate": 0.43548387096774194,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.016233766233766232,
            "MATH Mean Win Rate": 0.06451612903225806,
            "ethics_justice - EM": 0.7583333333333333,
            "ethics_justice - EM (Robustness)": 0.7,
            "ethics_justice - EM (Fairness)": 0.675,
            "ethics_commonsense - EM": 0.48333333333333334,
            "ethics_commonsense - EM (Robustness)": 0.36666666666666664,
            "ethics_commonsense - EM (Fairness)": 0.4166666666666667,
            "ethics_virtue - EM": 0.65,
            "ethics_virtue - EM (Robustness)": 0.6,
            "ethics_virtue - EM (Fairness)": 0.55,
            "ethics_deontology - EM": 0.6666666666666666,
            "ethics_deontology - EM (Robustness)": 0.5916666666666667,
            "ethics_deontology - EM (Fairness)": 0.6083333333333333,
            "ethics_utilitarianism - EM": 0.6416666666666667,
            "ethics_utilitarianism - EM (Robustness)": 0.4083333333333333,
            "ethics_utilitarianism - EM (Fairness)": 0.5166666666666667,
            "ethics Mean Win Rate": 0.5005376344086021,
            "Score": 0.31571390212823647
        }
    ],
    [
        "eric1236002_llama_recipes_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.016129032258064516,
            "sam_sum - ROUGE-2": 0.12837998124469352,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.3631725417439703,
            "sam_sum - Representation (race)": 0.45614035087719296,
            "sam_sum - Representation (gender)": 0.017857142857142877,
            "sam_sum Mean Win Rate": 0.4705069124423963,
            "corr2cause - EM": 0.475,
            "corr2cause Mean Win Rate": 0.43548387096774194,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.08008658008658008,
            "MATH Mean Win Rate": 0.7419354838709677,
            "ethics_justice - EM": 0.7583333333333333,
            "ethics_justice - EM (Robustness)": 0.7166666666666667,
            "ethics_justice - EM (Fairness)": 0.675,
            "ethics_commonsense - EM": 0.425,
            "ethics_commonsense - EM (Robustness)": 0.21666666666666667,
            "ethics_commonsense - EM (Fairness)": 0.3333333333333333,
            "ethics_virtue - EM": 0.8583333333333333,
            "ethics_virtue - EM (Robustness)": 0.7833333333333333,
            "ethics_virtue - EM (Fairness)": 0.7916666666666666,
            "ethics_deontology - EM": 0.6166666666666667,
            "ethics_deontology - EM (Robustness)": 0.5916666666666667,
            "ethics_deontology - EM (Fairness)": 0.6,
            "ethics_utilitarianism - EM": 0.7083333333333334,
            "ethics_utilitarianism - EM (Robustness)": 0.625,
            "ethics_utilitarianism - EM (Fairness)": 0.5416666666666666,
            "ethics Mean Win Rate": 0.6315770609318996,
            "Score": 0.27414970108937253
        }
    ],
    [
        "facico_nips_submit_2_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.17909924507908812,
            "CNN/DailyMail - Stereotypes (race)": 0.5161904761904762,
            "CNN/DailyMail - Stereotypes (gender)": 0.10250141418735452,
            "CNN/DailyMail - Representation (race)": 0.2284644194756554,
            "CNN/DailyMail - Representation (gender)": 0.0370675453047776,
            "CNN/DailyMail Mean Win Rate": 0.9419354838709677,
            "sam_sum - ROUGE-2": 0.009276116970054813,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.5,
            "sam_sum - Representation (race)": 0.6666666666666667,
            "sam_sum - Representation (gender)": 0.05445544554455442,
            "sam_sum Mean Win Rate": 0.0597187758478081,
            "corr2cause - EM": 0.0,
            "corr2cause Mean Win Rate": 0.02150537634408602,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.06570810142238713,
            "MATH Mean Win Rate": 0.6129032258064516,
            "ethics_justice - EM": 0.7083333333333334,
            "ethics_justice - EM (Robustness)": 0.65,
            "ethics_justice - EM (Fairness)": 0.65,
            "ethics_commonsense - EM": 0.35,
            "ethics_commonsense - EM (Robustness)": 0.30833333333333335,
            "ethics_commonsense - EM (Fairness)": 0.30833333333333335,
            "ethics_virtue - EM": 0.8916666666666667,
            "ethics_virtue - EM (Robustness)": 0.8666666666666667,
            "ethics_virtue - EM (Fairness)": 0.8416666666666667,
            "ethics_deontology - EM": 0.7,
            "ethics_deontology - EM (Robustness)": 0.6,
            "ethics_deontology - EM (Fairness)": 0.6416666666666667,
            "ethics_utilitarianism - EM": 0.5333333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.39166666666666666,
            "ethics_utilitarianism - EM (Fairness)": 0.4583333333333333,
            "ethics Mean Win Rate": 0.4876344086021505,
            "Score": 0.2049429910646362
        }
    ],
    [
        "facico_nips_submit_1_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1769097171194526,
            "CNN/DailyMail - Stereotypes (race)": 0.5612535612535612,
            "CNN/DailyMail - Stereotypes (gender)": 0.11560725292571047,
            "CNN/DailyMail - Representation (race)": 0.17439293598233996,
            "CNN/DailyMail - Representation (gender)": 0.029824561403508754,
            "CNN/DailyMail Mean Win Rate": 0.9290322580645161,
            "sam_sum - ROUGE-2": 0.00944320051172252,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.5,
            "sam_sum - Representation (race)": 0.6666666666666667,
            "sam_sum - Representation (gender)": 0.07843137254901963,
            "sam_sum Mean Win Rate": 0.03391232423490488,
            "corr2cause - EM": 0.0,
            "corr2cause Mean Win Rate": 0.02150537634408602,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.07235621521335807,
            "MATH Mean Win Rate": 0.6774193548387096,
            "ethics_justice - EM": 0.7166666666666667,
            "ethics_justice - EM (Robustness)": 0.6416666666666667,
            "ethics_justice - EM (Fairness)": 0.6333333333333333,
            "ethics_commonsense - EM": 0.4666666666666667,
            "ethics_commonsense - EM (Robustness)": 0.35833333333333334,
            "ethics_commonsense - EM (Fairness)": 0.45,
            "ethics_virtue - EM": 0.8833333333333333,
            "ethics_virtue - EM (Robustness)": 0.8666666666666667,
            "ethics_virtue - EM (Fairness)": 0.8333333333333334,
            "ethics_deontology - EM": 0.75,
            "ethics_deontology - EM (Robustness)": 0.5916666666666667,
            "ethics_deontology - EM (Fairness)": 0.5666666666666667,
            "ethics_utilitarianism - EM": 0.6833333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.5416666666666666,
            "ethics_utilitarianism - EM (Fairness)": 0.5916666666666667,
            "ethics Mean Win Rate": 0.6327240143369176,
            "Score": 0.1961559396219256
        }
    ],
    [
        "facico_nips_submit_3_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.1749733891531122,
            "CNN/DailyMail - Stereotypes (race)": 0.5598793363499247,
            "CNN/DailyMail - Stereotypes (gender)": 0.14008565497140768,
            "CNN/DailyMail - Representation (race)": 0.15942028985507245,
            "CNN/DailyMail - Representation (gender)": 0.030081300813008166,
            "CNN/DailyMail Mean Win Rate": 0.9225806451612903,
            "sam_sum - ROUGE-2": 0.009032136759061409,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.5,
            "sam_sum - Representation (race)": 0.6666666666666667,
            "sam_sum - Representation (gender)": 0.08585858585858591,
            "sam_sum Mean Win Rate": 0.014557485525227459,
            "corr2cause - EM": 0.0,
            "corr2cause Mean Win Rate": 0.02150537634408602,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.05272108843537415,
            "MATH Mean Win Rate": 0.4838709677419355,
            "ethics_justice - EM": 0.7083333333333334,
            "ethics_justice - EM (Robustness)": 0.65,
            "ethics_justice - EM (Fairness)": 0.65,
            "ethics_commonsense - EM": 0.0,
            "ethics_commonsense - EM (Robustness)": 0.0,
            "ethics_commonsense - EM (Fairness)": 0.0,
            "ethics_virtue - EM": 0.8916666666666667,
            "ethics_virtue - EM (Robustness)": 0.8666666666666667,
            "ethics_virtue - EM (Fairness)": 0.8416666666666667,
            "ethics_deontology - EM": 0.0,
            "ethics_deontology - EM (Robustness)": 0.0,
            "ethics_deontology - EM (Fairness)": 0.0,
            "ethics_utilitarianism - EM": 0.0,
            "ethics_utilitarianism - EM (Robustness)": 0.0,
            "ethics_utilitarianism - EM (Fairness)": 0.0,
            "ethics Mean Win Rate": 0.2467741935483871,
            "Score": 0.12809536395387736
        }
    ],
    [
        "fyzl233_llm_challenge_hidden",
        {
            "CNN/DailyMail - ROUGE-2": null,
            "CNN/DailyMail - Stereotypes (race)": null,
            "CNN/DailyMail - Stereotypes (gender)": null,
            "CNN/DailyMail - Representation (race)": null,
            "CNN/DailyMail - Representation (gender)": null,
            "CNN/DailyMail Mean Win Rate": 0.016129032258064516,
            "sam_sum - ROUGE-2": 0.048097179644997094,
            "sam_sum - Stereotypes (race)": null,
            "sam_sum - Stereotypes (gender)": 0.2417027417027417,
            "sam_sum - Representation (race)": 0.5,
            "sam_sum - Representation (gender)": 0.06023391812865497,
            "sam_sum Mean Win Rate": 0.2704714640198511,
            "corr2cause - EM": 0.5025,
            "corr2cause Mean Win Rate": 0.7580645161290323,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.0,
            "MATH Mean Win Rate": 0.016129032258064516,
            "ethics_justice - EM": 0.7333333333333333,
            "ethics_justice - EM (Robustness)": 0.675,
            "ethics_justice - EM (Fairness)": 0.575,
            "ethics_commonsense - EM": 0.475,
            "ethics_commonsense - EM (Robustness)": 0.36666666666666664,
            "ethics_commonsense - EM (Fairness)": 0.45,
            "ethics_virtue - EM": 0.75,
            "ethics_virtue - EM (Robustness)": 0.65,
            "ethics_virtue - EM (Fairness)": 0.6583333333333333,
            "ethics_deontology - EM": 0.75,
            "ethics_deontology - EM (Robustness)": 0.5666666666666667,
            "ethics_deontology - EM (Fairness)": 0.5333333333333333,
            "ethics_utilitarianism - EM": 0.6583333333333333,
            "ethics_utilitarianism - EM (Robustness)": 0.5166666666666667,
            "ethics_utilitarianism - EM (Fairness)": 0.5416666666666666,
            "ethics Mean Win Rate": 0.5340860215053763,
            "Score": 0.1232909192125279
        }
    ],
    [
        "neurips_llm_efficiency_challenge_sample_submissions_lit_gpt_hidden",
        {
            "CNN/DailyMail - ROUGE-2": 0.023454748286150778,
            "CNN/DailyMail - Stereotypes (race)": 0.6666666666666666,
            "CNN/DailyMail - Stereotypes (gender)": 0.45416666666666666,
            "CNN/DailyMail - Representation (race)": 0.3333333333333333,
            "CNN/DailyMail - Representation (gender)": 0.2564102564102564,
            "CNN/DailyMail Mean Win Rate": 0.289247311827957,
            "sam_sum - ROUGE-2": 0.03737378827183999,
            "sam_sum - Stereotypes (race)": 0.6666666666666667,
            "sam_sum - Stereotypes (gender)": 0.40435606060606055,
            "sam_sum - Representation (race)": 0.625,
            "sam_sum - Representation (gender)": 0.06697819314641743,
            "sam_sum Mean Win Rate": 0.1737327188940092,
            "corr2cause - EM": 0.435,
            "corr2cause Mean Win Rate": 0.0967741935483871,
            "MATH (chain-of-thoughts) - Equivalent (chain of thought)": 0.0,
            "MATH Mean Win Rate": 0.016129032258064516,
            "ethics_justice - EM": 0.55,
            "ethics_justice - EM (Robustness)": 0.55,
            "ethics_justice - EM (Fairness)": 0.55,
            "ethics_commonsense - EM": 0.016666666666666666,
            "ethics_commonsense - EM (Robustness)": 0.0,
            "ethics_commonsense - EM (Fairness)": 0.0,
            "ethics_virtue - EM": 0.8083333333333333,
            "ethics_virtue - EM (Robustness)": 0.8,
            "ethics_virtue - EM (Fairness)": 0.8083333333333333,
            "ethics_deontology - EM": 0.48333333333333334,
            "ethics_deontology - EM (Robustness)": 0.30833333333333335,
            "ethics_deontology - EM (Fairness)": 0.3,
            "ethics_utilitarianism - EM": 0.49166666666666664,
            "ethics_utilitarianism - EM (Robustness)": 0.36666666666666664,
            "ethics_utilitarianism - EM (Fairness)": 0.43333333333333335,
            "ethics Mean Win Rate": 0.20465949820788532,
            "Score": 0.10992846022794298
        }
    ]
]